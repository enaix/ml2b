# -*- coding: utf-8 -*-
"""ITMO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1855JITnh8kL8fEi1WSskrULUQ2hYmeIK
"""

!pip install catboost

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import lightgbm as lgb
from catboost import CatBoostRegressor
import warnings
warnings.filterwarnings('ignore')

import sys
sys.path.insert(0, './')


class RealEstatePricePredictor:
    def __init__(self, stratified_split=False):
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.models = {}
        self.best_model = None
        self.feature_names = None
        self.stratified_split = stratified_split

    def load_data(self, train_path):
        """Load training dataset"""
        self.train_df = pd.read_csv(train_path)
        if 'Unnamed: 0' in self.train_df.columns:
            self.train_df = self.train_df.drop('Unnamed: 0', axis=1)
        if 'index' in self.train_df.columns:
            self.train_df = self.train_df.drop('index', axis=1)
        print(f"Train shape: {self.train_df.shape}")
        return self.train_df

    def eda(self):
        """Exploratory Data Analysis"""
        print("\n=== Data Info ===")
        print(self.train_df.info())

        print("\n=== Statistical Summary ===")
        print(self.train_df.describe())

        print("\n=== Missing Values ===")
        missing = self.train_df.isnull().sum()
        if missing.sum() > 0:
            print(missing[missing > 0])
        else:
            print("No missing values")

        print("\n=== Target Distribution ===")
        print(f"Mean price: {self.train_df['price'].mean():.2f}")
        print(f"Median price: {self.train_df['price'].median():.2f}")
        print(f"Std price: {self.train_df['price'].std():.2f}")
        print(f"Min price: {self.train_df['price'].min():.2f}")
        print(f"Max price: {self.train_df['price'].max():.2f}")

        print("\n=== Categorical Features ===")
        cat_cols = self.train_df.select_dtypes(include=['object']).columns
        for col in cat_cols:
            if col != 'price':
                print(f"\n{col}:")
                print(self.train_df[col].value_counts().head())

    def feature_engineering(self, df, is_train=True):
        """Create new features from existing ones"""
        df = df.copy()

        cat_cols = ['gas', 'hot_water', 'central_heating', 'extra_area_type_name', 'district_name']
        for col in cat_cols:
            if col in df.columns:
                if is_train:
                    self.label_encoders[col] = LabelEncoder()
                    df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].fillna('Unknown'))
                else:
                    df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col].fillna('Unknown'))

        df['total_room_area'] = df['kitchen_area'] + df['bath_area'] + df['other_area'].fillna(0)
        df['area_per_room'] = df['total_area'] / (df['rooms_count'] + 1e-6)
        df['kitchen_ratio'] = df['kitchen_area'] / (df['total_area'] + 1e-6)
        df['bath_ratio'] = df['bath_area'] / (df['total_area'] + 1e-6)

        df['has_extra_area'] = (df['extra_area'] > 0).astype(int)
        df['extra_area_per_count'] = df['extra_area'] / (df['extra_area_count'] + 1e-6)

        df['building_age'] = 2024 - df['year']
        df['floor_ratio'] = df['floor'] / (df['floor_max'] + 1e-6)
        df['is_first_floor'] = (df['floor'] == 1).astype(int)
        df['is_last_floor'] = (df['floor'] == df['floor_max']).astype(int)
        df['middle_floor'] = ((df['floor'] > 1) & (df['floor'] < df['floor_max'])).astype(int)

        df['bath_per_room'] = df['bath_count'] / (df['rooms_count'] + 1e-6)
        df['area_per_bath'] = df['total_area'] / (df['bath_count'] + 1e-6)

        df['has_gas'] = (df['gas'] == 'yes').astype(int) if 'gas' in df.columns else 0
        df['has_hot_water'] = (df['hot_water'] == 'yes').astype(int) if 'hot_water' in df.columns else 0
        df['has_heating'] = (df['central_heating'] == 'yes').astype(int) if 'central_heating' in df.columns else 0
        df['amenities_score'] = df['has_gas'] + df['has_hot_water'] + df['has_heating']

        df['total_area_squared'] = df['total_area'] ** 2
        df['rooms_count_squared'] = df['rooms_count'] ** 2
        df['building_age_squared'] = df['building_age'] ** 2

        df['area_rooms_interaction'] = df['total_area'] * df['rooms_count']
        df['area_floor_interaction'] = df['total_area'] * df['floor_ratio']
        df['area_age_interaction'] = df['total_area'] * df['building_age']
        df['rooms_age_interaction'] = df['rooms_count'] * df['building_age']

        df['log_total_area'] = np.log1p(df['total_area'])
        df['log_kitchen_area'] = np.log1p(df['kitchen_area'])
        df['log_building_age'] = np.log1p(df['building_age'])

        df['volume'] = df['total_area'] * df['ceil_height']
        df['log_volume'] = np.log1p(df['volume'])

        return df

    def prepare_features(self):
      """Prepare features for modeling with train/val split"""

      train_engineered = self.feature_engineering(self.train_df, is_train=True)

      drop_cols = ['price', 'gas', 'hot_water', 'central_heating',
                  'extra_area_type_name', 'district_name']

      feature_cols = [col for col in train_engineered.columns if col not in drop_cols]

      X = train_engineered[feature_cols]
      y = train_engineered['price']

      self.feature_names = feature_cols

      # === Stratification Fix ===
      # train_test_split cannot stratify continuous y
      # so we create quantile bins only if stratified_split=True

      if self.stratified_split:
          # 10 quantile bins for balanced stratification
          y_mapped = pd.qcut(y, q=10, labels=False, duplicates="drop")
      else:
          y_mapped = y

      # === REQUIRED SPLIT (exactly your version) ===
      X_train, X_val, y_train_mapped, y_val_mapped = train_test_split(
          X, y_mapped,
          test_size=0.2,
          random_state=42,
          shuffle=True,
          stratify=y_mapped if self.stratified_split else None
      )

      # We restore TRUE continuous target values after splitting
      y_train = y.loc[X_train.index]
      y_val = y.loc[X_val.index]

      print(f"\n=== Data Split ===")
      print(f"X_train: {X_train.shape}, X_val: {X_val.shape}")
      print(f"Features: {len(feature_cols)}")

      # Standard Scaling
      X_train_scaled = self.scaler.fit_transform(X_train)
      X_val_scaled = self.scaler.transform(X_val)

      # Back to DataFrame with original column names & indices
      X_train_scaled = pd.DataFrame(X_train_scaled,
                                    columns=feature_cols,
                                    index=X_train.index)
      X_val_scaled = pd.DataFrame(X_val_scaled,
                                  columns=feature_cols,
                                  index=X_val.index)

      return X_train_scaled, X_val_scaled, y_train, y_val


    def train_models(self, X_train, X_val, y_train, y_val):
        """Train regression models"""
        models = {
            'Ridge': Ridge(alpha=100.0, random_state=42),
            'Lasso': Lasso(alpha=10.0, random_state=42, max_iter=2000),
            'RandomForest': RandomForestRegressor(
                n_estimators=200, max_depth=15, min_samples_split=5,
                min_samples_leaf=2, random_state=42, n_jobs=-1
            ),
            'GradientBoosting': GradientBoostingRegressor(
                n_estimators=200, learning_rate=0.1, max_depth=5,
                min_samples_split=5, min_samples_leaf=2,
                subsample=0.8, random_state=42
            ),
            'LightGBM': lgb.LGBMRegressor(
                n_estimators=200, learning_rate=0.1, max_depth=7,
                num_leaves=31, min_child_samples=20, subsample=0.8,
                colsample_bytree=0.8, random_state=42, verbose=-1
            ),
            'CatBoost': CatBoostRegressor(
                iterations=200, learning_rate=0.1, depth=6,
                l2_leaf_reg=3, random_seed=42, verbose=False
            )
        }

        results = {}

        print("\n=== Training Models ===")
        for name, model in models.items():
            print(f"\n{name}...")
            model.fit(X_train, y_train)

            y_train_pred = model.predict(X_train)
            y_val_pred = model.predict(X_val)

            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
            val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
            val_mae = mean_absolute_error(y_val, y_val_pred)
            val_r2 = r2_score(y_val, y_val_pred)

            results[name] = {
                'model': model,
                'train_rmse': train_rmse,
                'val_rmse': val_rmse,
                'val_mae': val_mae,
                'val_r2': val_r2,
                'y_val_pred': y_val_pred
            }

            print(f"  Train RMSE: {train_rmse:.2f} | Val RMSE: {val_rmse:.2f}")
            print(f"  Val MAE: {val_mae:.2f} | Val R2: {val_r2:.4f}")

        best_model_name = min(results.keys(), key=lambda x: results[x]['val_rmse'])
        self.best_model = results[best_model_name]['model']
        self.models = results

        print(f"\n{'='*50}")
        print(f"Best Model: {best_model_name}")
        print(f"Val RMSE: {results[best_model_name]['val_rmse']:.2f}")
        print(f"{'='*50}")

        return results, best_model_name

    def create_ensemble(self, X_val, y_val, top_n=3):
        """Create weighted ensemble"""
        val_predictions = {}
        for name, model_dict in self.models.items():
            val_predictions[name] = model_dict['y_val_pred']

        top_models = sorted(self.models.items(), key=lambda x: x[1]['val_rmse'])[:top_n]

        print(f"\n=== Ensemble (Top {top_n} - Average) ===")

        ensemble_pred = np.zeros(len(y_val))
        for name, model_dict in top_models:
            ensemble_pred += val_predictions[name]
            print(f"  {name}: RMSE={model_dict['val_rmse']:.2f}")
        ensemble_pred /= top_n

        ensemble_rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))
        ensemble_mae = mean_absolute_error(y_val, ensemble_pred)
        ensemble_r2 = r2_score(y_val, ensemble_pred)

        print(f"\n  Ensemble RMSE: {ensemble_rmse:.2f}")
        print(f"  Ensemble MAE: {ensemble_mae:.2f}")
        print(f"  Ensemble R2: {ensemble_r2:.4f}")

        return ensemble_rmse, top_models, ensemble_pred

    def validate_with_grader(self, y_val, y_pred):
      """Validate predictions using internal RMSE metric (no external grader)"""
      rmse = np.sqrt(np.mean((y_val - y_pred) ** 2))

      print("\n=== Validation ===")
      print(f"RMSE: {rmse:.2f}")

      return rmse

    def get_feature_importance(self, top_n=20):
        """Get feature importance from best tree-based model"""
        if hasattr(self.best_model, 'feature_importances_'):
            importances = self.best_model.feature_importances_
            feature_imp = pd.DataFrame({
                'feature': self.feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)

            print(f"\n=== Top {top_n} Important Features ===")
            print(feature_imp.head(top_n).to_string(index=False))
            return feature_imp
        else:
            print("\nBest model does not have feature_importances_")
            return None


def main():
    print("="*70)
    print("REAL ESTATE PRICE PREDICTION")
    print("Metric: RMSE (grader_default)")
    print("="*70)

    predictor = RealEstatePricePredictor(stratified_split=False)

    train_df = predictor.load_data('train.csv')

    predictor.eda()

    X_train, X_val, y_train, y_val = predictor.prepare_features()

    results, best_model_name = predictor.train_models(X_train, X_val, y_train, y_val)

    best_pred = results[best_model_name]['y_val_pred']
    best_grader_score = predictor.validate_with_grader(y_val, best_pred)

    ensemble_rmse, top_models, ensemble_pred = predictor.create_ensemble(X_val, y_val, top_n=3)

    ensemble_grader_score = predictor.validate_with_grader(y_val, ensemble_pred)

    feature_imp = predictor.get_feature_importance(top_n=20)

    print("\n" + "="*70)
    print("FINAL SUMMARY")
    print("="*70)
    print(f"Best Single Model: {best_model_name}")
    print(f"  Sklearn RMSE: {results[best_model_name]['val_rmse']:.2f}")
    print(f"  Grader Score: {best_grader_score:.2f}")
    print(f"\nEnsemble Model:")
    print(f"  Sklearn RMSE: {ensemble_rmse:.2f}")
    print(f"  Grader Score: {ensemble_grader_score:.2f}")

    improvement = results[best_model_name]['val_rmse'] - ensemble_rmse
    if improvement > 0:
        print(f"\nEnsemble improvement: -{improvement:.2f} RMSE")
    else:
        print(f"\nSingle model wins")

    print("="*70)

    return predictor, results


if __name__ == "__main__":
    predictor, results = main()