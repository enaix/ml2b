# -*- coding: utf-8 -*-
"""beer_quality_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17WQLxu46abIFQd_INZma9Bcj7xETsXji
"""

!pip install catboost

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
import warnings
warnings.filterwarnings('ignore')

class BeerQualityClassifier:
    def __init__(self, stratified_split=False):
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.models = {}
        self.best_model = None
        self.feature_names = None
        self.stratified_split = stratified_split

    def load_data(self, train_path):
        """Load training dataset"""
        self.train_df = pd.read_csv(train_path)
        # Drop unnamed index column if it exists
        if 'Unnamed: 0' in self.train_df.columns:
            self.train_df = self.train_df.drop('Unnamed: 0', axis=1)
        print(f"Train shape: {self.train_df.shape}")
        return self.train_df

    def eda(self):
        """Exploratory Data Analysis"""
        print("\n=== Data Info ===")
        print(self.train_df.info())

        print("\n=== Statistical Summary ===")
        print(self.train_df.describe())

        print("\n=== Missing Values ===")
        print(self.train_df.isnull().sum())

        print("\n=== Target Distribution (Quality Classes) ===")
        print(self.train_df['quality'].value_counts().sort_index())
        print(f"\nNumber of classes: {self.train_df['quality'].nunique()}")

        print("\n=== Beer Styles ===")
        print(self.train_df['beer_color_category'].value_counts())

        # Correlation with target
        numeric_cols = self.train_df.select_dtypes(include=[np.number]).columns
        correlations = self.train_df[numeric_cols].corr()['quality'].sort_values(ascending=False)
        print("\n=== Top Correlations with Quality ===")
        print(correlations)

    def feature_engineering(self, df, is_train=True):
        """Create new features from existing ones"""
        df = df.copy()

        # Encode categorical feature (beer_style)
        if is_train:
            df['beer_style_encoded'] = self.label_encoder.fit_transform(df['beer_color_category'])
        else:
            df['beer_style_encoded'] = self.label_encoder.transform(df['beer_color_category'])

        # Gravity-based features
        df['gravity_difference'] = df['original_gravity'] - df['final_gravity']
        df['attenuation'] = (df['original_gravity'] - df['final_gravity']) / (df['original_gravity'] + 1e-6)

        # CO2 and oxygen balance
        df['CO2_oxygen_ratio'] = df['free_CO2'] / (df['dissolved_oxygen'] + 1e-6)
        df['carbonation_balance'] = df['free_CO2'] - df['dissolved_oxygen']

        # Flavor complexity
        df['bitterness_alcohol_ratio'] = df['bitterness_IBU'] / (df['alcohol_ABV'] + 1e-6)
        df['flavor_complexity'] = df['bitterness_IBU'] * df['diacetyl_concentration'] * df['lactic_acid']

        # Chemical balance
        df['acid_mineral_balance'] = df['lactic_acid'] / (df['gypsum_level'] + 1e-6)
        df['sodium_pH_interaction'] = df['sodium'] * df['pH']

        # Fermentation metrics
        df['fermentation_efficiency'] = df['alcohol_ABV'] / (df['fermentation_strength'] + 1e-6)
        df['fermentation_gravity_product'] = df['fermentation_strength'] * df['gravity_difference']

        # Quality indicators
        df['sweetness_indicator'] = df['final_gravity'] / (df['alcohol_ABV'] + 1e-6)
        df['off_flavor_index'] = df['diacetyl_concentration'] + df['dissolved_oxygen']

        # Polynomial features for key variables
        df['bitterness_squared'] = df['bitterness_IBU'] ** 2
        df['alcohol_squared'] = df['alcohol_ABV'] ** 2
        df['pH_squared'] = df['pH'] ** 2

        # Interaction terms
        df['bitter_alcohol'] = df['bitterness_IBU'] * df['alcohol_ABV']
        df['gravity_alcohol'] = df['original_gravity'] * df['alcohol_ABV']
        df['CO2_fermentation'] = df['free_CO2'] * df['fermentation_strength']

        # Log transformations for skewed features
        df['log_bitterness'] = np.log1p(df['bitterness_IBU'])
        df['log_diacetyl'] = np.log1p(df['diacetyl_concentration'])
        df['log_dissolved_oxygen'] = np.log1p(df['dissolved_oxygen'])

        # Additional ratios
        df['gravity_strength_ratio'] = df['original_gravity'] / (df['fermentation_strength'] + 1e-6)
        df['CO2_gravity_ratio'] = df['free_CO2'] / (df['final_gravity'] + 1e-6)

        return df

    def prepare_features(self):
        """Prepare features for modeling with train/val split"""
        # Apply feature engineering
        train_engineered = self.feature_engineering(self.train_df, is_train=True)

        # Drop unnecessary columns
        drop_cols = ['id', 'beer_style', 'quality']
        feature_cols = [col for col in train_engineered.columns if col not in drop_cols]

        X = train_engineered[feature_cols]
        y = train_engineered['quality']

        self.feature_names = feature_cols
        self.classes = sorted(y.unique())

        # Create mapping for XGBoost (needs 0-indexed classes)
        self.class_mapping = {cls: idx for idx, cls in enumerate(self.classes)}
        self.inverse_class_mapping = {idx: cls for cls, idx in self.class_mapping.items()}

        # Remap y to 0-indexed
        y_mapped = y.map(self.class_mapping)

        # Split exactly as specified
        X_train, X_val, y_train, y_val = train_test_split(
            X, y_mapped,
            test_size=0.2,
            random_state=42,
            shuffle=True,
            stratify=y_mapped if self.stratified_split else None
        )

        print(f"\n=== Data Split ===")
        print(f"X_train shape: {X_train.shape}")
        print(f"X_val shape: {X_val.shape}")
        print(f"y_train shape: {y_train.shape}")
        print(f"y_val shape: {y_val.shape}")
        print(f"Number of features: {len(feature_cols)}")
        print(f"Number of classes: {len(self.classes)}")
        print(f"Original classes: {self.classes}")
        print(f"Mapped to: {list(range(len(self.classes)))}")

        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)

        # Convert back to DataFrame for tree-based models
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)
        X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_cols, index=X_val.index)

        # Store original y_val for reporting
        self.y_val_original = y_val.map(self.inverse_class_mapping)

        return X_train_scaled, X_val_scaled, y_train, y_val

    def train_models(self, X_train, X_val, y_train, y_val):
        """Train multiple classification models and select the best one"""

        # Define models
        models = {
            'LogisticRegression': LogisticRegression(
                max_iter=1000,
                random_state=42,
                multi_class='multinomial'
            ),
            'RandomForest': RandomForestClassifier(
                n_estimators=200,
                max_depth=12,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            'GradientBoosting': GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=5,
                min_samples_split=5,
                min_samples_leaf=2,
                subsample=0.8,
                random_state=42
            ),
            'XGBoost': xgb.XGBClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=6,
                min_child_weight=3,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                tree_method='hist',
                eval_metric='mlogloss'
            ),
            'LightGBM': lgb.LGBMClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=7,
                num_leaves=31,
                min_child_samples=20,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                verbose=-1
            ),
            'CatBoost': CatBoostClassifier(
                iterations=200,
                learning_rate=0.1,
                depth=6,
                l2_leaf_reg=3,
                random_seed=42,
                verbose=False
            )
        }

        results = {}

        print("\n=== Training Classification Models ===")
        for name, model in models.items():
            print(f"\nTraining {name}...")

            # Train
            model.fit(X_train, y_train)

            # Predictions
            y_train_pred = model.predict(X_train)
            y_val_pred = model.predict(X_val)

            # Metrics - CATEGORIZATION ACCURACY
            train_acc = accuracy_score(y_train, y_train_pred)
            val_acc = accuracy_score(y_val, y_val_pred)

            results[name] = {
                'model': model,
                'train_accuracy': train_acc,
                'val_accuracy': val_acc,
                'y_val_pred': y_val_pred
            }

            print(f"  Train Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)")
            print(f"  Val Accuracy:   {val_acc:.4f} ({val_acc*100:.2f}%)")

        # Select best model based on validation accuracy
        best_model_name = max(results.keys(), key=lambda x: results[x]['val_accuracy'])
        self.best_model = results[best_model_name]['model']
        self.models = results

        print(f"\n{'='*50}")
        print(f"Best Model: {best_model_name}")
        print(f"   Val Accuracy: {results[best_model_name]['val_accuracy']:.4f} ({results[best_model_name]['val_accuracy']*100:.2f}%)")
        print(f"{'='*50}")

        return results, best_model_name

    def create_ensemble(self, X_val, y_val, top_n=3):
        """Create voting ensemble of top models"""
        from scipy import stats

        # Get predictions from all models
        val_predictions = {}
        for name, model_dict in self.models.items():
            model = model_dict['model']
            val_predictions[name] = model.predict(X_val)

        # Top N models by validation accuracy
        top_models = sorted(self.models.items(), key=lambda x: x[1]['val_accuracy'], reverse=True)[:top_n]

        print(f"\n=== Ensemble (Top {top_n} Models - Voting) ===")

        # Collect predictions from top models
        ensemble_preds = []
        for name, model_dict in top_models:
            ensemble_preds.append(val_predictions[name])
            print(f"  Including: {name} (Val Accuracy: {model_dict['val_accuracy']:.4f})")

        # Majority voting
        ensemble_preds = np.array(ensemble_preds)
        ensemble_pred = stats.mode(ensemble_preds, axis=0, keepdims=False)[0]

        ensemble_acc = accuracy_score(y_val, ensemble_pred)

        print(f"\n  Ensemble Val Accuracy: {ensemble_acc:.4f} ({ensemble_acc*100:.2f}%)")

        return ensemble_acc, top_models, ensemble_pred

    def detailed_evaluation(self, y_val, y_pred, model_name):
        """Detailed classification metrics"""
        print(f"\n=== Detailed Evaluation: {model_name} ===")

        # Categorization Accuracy
        accuracy = accuracy_score(y_val, y_pred)
        print(f"\nCategorization Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

        # Classification Report
        print("\n Classification Report:")
        print(classification_report(y_val, y_pred, zero_division=0))

        # Confusion Matrix
        print("\n Confusion Matrix:")
        cm = confusion_matrix(y_val, y_pred)

        # Create nice formatted confusion matrix
        classes = sorted(y_val.unique())
        cm_df = pd.DataFrame(cm, index=[f'True {c}' for c in classes],
                            columns=[f'Pred {c}' for c in classes])
        print(cm_df)

        # Per-class accuracy
        print("\n Per-Class Accuracy:")
        for i, cls in enumerate(classes):
            class_mask = y_val == cls
            if class_mask.sum() > 0:
                class_acc = (y_pred[class_mask] == cls).sum() / class_mask.sum()
                print(f"  Class {cls}: {class_acc:.4f} ({class_acc*100:.2f}%)")

    def get_feature_importance(self, top_n=20):
        """Get feature importance from best tree-based model"""
        if hasattr(self.best_model, 'feature_importances_'):
            importances = self.best_model.feature_importances_
            feature_imp = pd.DataFrame({
                'feature': self.feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)

            print(f"\n=== Top {top_n} Important Features ===")
            print(feature_imp.head(top_n).to_string(index=False))
            return feature_imp
        else:
            print("\nBest model doesn't have feature_importances_")
            return None

    def summary(self, results):
        """Print summary of all models"""
        print("\n" + "="*70)
        print("ðŸ“Š MODEL COMPARISON SUMMARY (CATEGORIZATION ACCURACY)")
        print("="*70)

        summary_df = pd.DataFrame({
            'Model': list(results.keys()),
            'Train Accuracy': [results[m]['train_accuracy'] for m in results.keys()],
            'Val Accuracy': [results[m]['val_accuracy'] for m in results.keys()],
            'Train %': [results[m]['train_accuracy']*100 for m in results.keys()],
            'Val %': [results[m]['val_accuracy']*100 for m in results.keys()]
        }).sort_values('Val Accuracy', ascending=False)

        print(summary_df.to_string(index=False))
        print("="*70)


def main():
    # Initialize classifier
    # Set stratified_split=True for stratified split (recommended for classification)
    classifier = BeerQualityClassifier(stratified_split=False)

    # Load data
    train_df = classifier.load_data('./train.csv')

    # EDA
    classifier.eda()

    # Prepare features with train/val split
    X_train, X_val, y_train, y_val = classifier.prepare_features()

    # Train models
    results, best_model_name = classifier.train_models(X_train, X_val, y_train, y_val)

    # Create ensemble
    ensemble_acc, top_models, ensemble_pred = classifier.create_ensemble(X_val, y_val, top_n=3)

    # Detailed evaluation for best model
    best_pred = results[best_model_name]['y_val_pred']
    classifier.detailed_evaluation(y_val, best_pred, best_model_name)

    # Detailed evaluation for ensemble
    classifier.detailed_evaluation(y_val, ensemble_pred, "Ensemble")

    # Feature importance
    feature_imp = classifier.get_feature_importance(top_n=20)

    # Print summary
    classifier.summary(results)

    return classifier, results, X_train, X_val, y_train, y_val


if __name__ == "__main__":
    classifier, results, X_train, X_val, y_train, y_val = main()