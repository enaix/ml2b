<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[1, 6], [4, 5], [6, 7], [7, 8], [8, 9], [9, 10], [10, 65], [10, 47], [10, 44], [10, 25], [10, 52], [10, 61], [10, 64], [10, 62], [10, 20], [10, 29], [10, 33], [10, 19], [10, 35], [10, 21], [10, 51], [10, 37], [10, 53], [10, 45], [10, 59], [10, 24], [10, 48], [10, 11], [10, 57], [10, 30], [10, 38], [10, 13], [10, 41], [10, 22], [10, 39], [10, 26], [10, 42], [10, 55], [10, 16], [10, 27], [10, 43], [10, 49], [10, 31], [10, 18], [10, 28], [11, 12], [13, 14], [14, 15], [16, 17], [22, 23], [31, 32], [33, 34], [35, 36], [39, 40], [45, 46], [49, 50], [53, 54], [55, 56], [57, 58], [59, 60], [62, 63], [65, 66], [66, 76], [66, 74], [66, 69], [66, 68], [66, 73], [66, 72], [66, 67], [66, 71], [69, 70], [74, 75]], "layout": [[0.43373493975903615, 0.0], [0.4578313253012048, 0.0], [0.4819277108433735, 0.0], [0.5060240963855421, 0.0], [0.5301204819277109, 0.0], [0.5301204819277109, 0.11111111111111116], [0.4578313253012048, 0.11111111111111116], [0.4578313253012048, 0.2222222222222222], [0.4578313253012048, 0.33333333333333337], [0.4578313253012048, 0.4444444444444444], [0.4578313253012048, 0.5555555555555556], [0.0, 0.6666666666666667], [0.0, 0.7777777777777778], [0.024096385542168676, 0.6666666666666667], [0.024096385542168676, 0.7777777777777778], [0.024096385542168676, 0.8888888888888888], [0.04819277108433735, 0.6666666666666667], [0.04819277108433735, 0.7777777777777778], [0.07228915662650602, 0.6666666666666667], [0.0963855421686747, 0.6666666666666667], [0.12048192771084337, 0.6666666666666667], [0.14457831325301204, 0.6666666666666667], [0.1686746987951807, 0.6666666666666667], [0.1686746987951807, 0.7777777777777778], [0.1927710843373494, 0.6666666666666667], [0.21686746987951808, 0.6666666666666667], [0.24096385542168675, 0.6666666666666667], [0.26506024096385544, 0.6666666666666667], [0.2891566265060241, 0.6666666666666667], [0.3132530120481928, 0.6666666666666667], [0.3373493975903614, 0.6666666666666667], [0.3614457831325301, 0.6666666666666667], [0.3614457831325301, 0.7777777777777778], [0.3855421686746988, 0.6666666666666667], [0.3855421686746988, 0.7777777777777778], [0.40963855421686746, 0.6666666666666667], [0.40963855421686746, 0.7777777777777778], [0.43373493975903615, 0.6666666666666667], [0.4578313253012048, 0.6666666666666667], [0.4819277108433735, 0.6666666666666667], [0.4819277108433735, 0.7777777777777778], [0.5060240963855421, 0.6666666666666667], [0.5301204819277109, 0.6666666666666667], [0.5542168674698795, 0.6666666666666667], [0.5783132530120482, 0.6666666666666667], [0.6024096385542169, 0.6666666666666667], [0.6024096385542169, 0.7777777777777778], [0.6265060240963856, 0.6666666666666667], [0.6506024096385542, 0.6666666666666667], [0.6746987951807228, 0.6666666666666667], [0.6746987951807228, 0.7777777777777778], [0.6987951807228916, 0.6666666666666667], [0.7228915662650602, 0.6666666666666667], [0.7469879518072289, 0.6666666666666667], [0.7469879518072289, 0.7777777777777778], [0.7710843373493976, 0.6666666666666667], [0.7710843373493976, 0.7777777777777778], [0.7951807228915663, 0.6666666666666667], [0.7951807228915663, 0.7777777777777778], [0.8192771084337349, 0.6666666666666667], [0.8192771084337349, 0.7777777777777778], [0.8433734939759037, 0.6666666666666667], [0.8674698795180723, 0.6666666666666667], [0.8674698795180723, 0.7777777777777778], [0.891566265060241, 0.6666666666666667], [0.9156626506024096, 0.6666666666666667], [0.9156626506024096, 0.7777777777777778], [0.8313253012048193, 0.8888888888888888], [0.8554216867469879, 0.8888888888888888], [0.8795180722891566, 0.8888888888888888], [0.8795180722891566, 1.0], [0.9036144578313253, 0.8888888888888888], [0.927710843373494, 0.8888888888888888], [0.9518072289156626, 0.8888888888888888], [0.9759036144578314, 0.8888888888888888], [0.9759036144578314, 1.0], [1.0, 0.8888888888888888]], "plan": ["The plan is to read the training data, split it into a training and validation\nset, and treat all object columns as categorical features. We\u2019ll convert these\ncolumns to the pandas \u201ccategory\u201d dtype and train a LightGBM regressor, which\nnatively handles categorical data. The `train` function fits the model,\n`prepare_val` simply ensures the validation features have the same categorical\ndtypes, and `predict` returns the model\u2019s predictions. Finally, we compute and\nprint the RMSE on the validation split.", "A straightforward approach is to treat all object columns as categorical\nfeatures, convert them to the pandas \u201ccategory\u201d dtype, and train a LightGBM\nregressor which natively handles such data. The `train` function fits the model\non the training split, `prepare_val` aligns the validation data\u2019s categorical\ntypes with the training data, and `predict` returns the model\u2019s predictions.\nWe\u2019ll split the provided training set into an 80/20 train\u2011validation split,\ncompute the RMSE on the validation part, print it, and expose the required\n`train`, `prepare_val`, `predict`, and `run` functions for reuse.", "The plan is to load the training CSV, split it into a training and validation\nset, and treat all object columns as categorical features. We will factorize\nthese categorical columns to integer codes, train a LightGBM regressor on the\ntraining split, and then apply the same factorization to the validation data for\nprediction. Finally, we compute and print the RMSE on the validation set. The\nrequired `train`, `prepare_val`, `predict`, and `run` functions are implemented\nto follow the competition interface.", "We will treat all object columns as categorical, factorize them to integer codes\nwhile keeping the mapping from the training set. A LightGBM regressor (which\nnatively handles categorical features) will be trained on the training split.\nThe same mappings are applied to the validation features before predicting. RMSE\non the hold\u2011out validation set is printed, and if a `test.csv` exists in\n`./input` the model is retrained on the full training data and predictions are\nsaved to `./working/submission.csv`.", "The plan is to treat all object columns as categorical, encode them with pandas\ncategorical dtypes and train a LightGBM regressor which natively handles these\ncategories. We split the provided training set into a hold\u2011out validation split,\nfit the model on the training part, and compute RMSE on the validation part,\nprinting the result. If a `test.csv` file exists we retrain on the full training\ndata and write predictions to `./working/submission.csv`. The required `train`,\n`prepare_val`, `predict`, and `run` functions are implemented to follow the\ncompetition interface.", "The bug arose because `LGBMRegressor.fit` does not accept a `verbose` argument,\ncausing a TypeError. We remove this unsupported parameter and keep the rest of\nthe LightGBM setup unchanged. The training, validation preprocessing, and\nprediction functions retain their required signatures, and the script now loads\nthe data, evaluates RMSE on a hold\u2011out split, prints the metric, and if a test\nset exists, trains on the full data and writes predictions to\n`./working/submission.csv`. All steps are self\u2011contained in a single file.", "The bug arose because `lgb.train` was called with a `verbose_eval` argument that\nisn\u2019t supported in the installed LightGBM version. Switching to the scikit\u2011learn\nAPI (`lgb.LGBMRegressor`) removes that issue and still handles categorical\nfeatures. The fix keeps the required `train`, `prepare_val`, `predict`, and\n`run` functions, casts object columns to the categorical dtype, passes the\ncategorical column list to the model, and computes RMSE on a validation split.\nIf a test file is present, predictions are saved to `./working/submission.csv`.", "We can boost robustness by replacing the single train/validation split with\n5\u2011fold cross\u2011validation, training a LightGBM model with early stopping on each\nfold and aggregating the out\u2011of\u2011fold predictions to compute a more reliable\nRMSE. After cross\u2011validation we retrain on the full training set and generate\ntest predictions, saving them to `./working/submission.csv`. This change keeps\nthe required API functions while improving evaluation stability.", "The issue arose because `early_stopping_rounds` is not a valid argument for the\nscikit\u2011learn API of LightGBM in the installed version, causing a `TypeError`.\nThe fix removes this argument and relies on a plain fit, which still yields a\nstrong model. The script now performs 5\u2011fold K\u2011Fold cross\u2011validation (shuffling\nwith a fixed seed), computes the out\u2011of\u2011fold RMSE, prints it, retrains on the\nfull training set, and if a test file exists, creates a `submission.csv` with\npredictions in the required `./working` directory. The required `train`,\n`prepare_val`, `predict`, and `run` functions are retained for modular use.", "We add early stopping to the LightGBM training inside each CV fold, using the\nvalidation split as an evaluation set. This lets the model stop before\nover\u2011fitting and typically yields a lower RMSE. The `train` function now\nreceives the validation data, passes it to `fit` with\n`early_stopping_rounds=100`, and we keep the same categorical handling. The rest\nof the pipeline (pre\u2011processing, prediction, submission) stays unchanged, and\nthe script still prints the 5\u2011fold OOF RMSE.", "The bug arose because `early_stopping_rounds` was passed as a keyword argument\nto `LGBMRegressor.fit`, which some LightGBM versions don\u2019t accept that way. The\nfix is to remove early\u2011stopping logic and train the model directly on the full\ndata (or on each fold without early stopping). The rest of the\npipeline\u2014categorical handling, 5\u2011fold CV, RMSE reporting, and test\u2011set\nprediction\u2014remains unchanged.", "We add early\u2011stopping to the LightGBM training: each CV fold\u2019s training set is\nsplit into a small internal validation set (10\u202f% of the fold) and the model is\nfit with `early_stopping_rounds=100`. This lets the boosting stop once\nperformance stops improving, reducing over\u2011fitting and typically lowering the\nRMSE. The rest of the pipeline (categorical handling, final retraining on all\ndata, and test\u2011set prediction) stays unchanged.", "The bug occurred because the LightGBM\u202f`LGBMRegressor.fit` call was given the\nargument `early_stopping_rounds`, which isn\u2019t supported by the installed\nLightGBM version. The fix is to train the model without early\u2011stopping (simply\ncalling `fit`), while still passing the categorical column list. The required\nAPI functions (`train`, `prepare_val`, `predict`, `run`) are kept unchanged, and\na separate script section performs 5\u2011fold cross\u2011validation, prints the RMSE,\nretrains on the full data, and writes test predictions to\n`./working/submission.csv` if a test file is present.", "We can improve the model by applying a log\u2011transform to the target, which often\nstabilises variance and helps LightGBM fit better, and by adding early stopping\non a small internal validation split so the model stops before over\u2011fitting\nwhile still using a large `n_estimators`. Both changes are lightweight and can\nbe added inside the existing `train`/`predict` pipeline.", "The issue arose because the LightGBM\u202fsklearn API version in the environment does\nnot accept the `early_stopping_rounds` argument in `LGBMRegressor.fit`. The fix\nis to remove this argument and rely on a fixed number of trees (or use\nLightGBM\u2019s callback API, but a simple fixed\u2011tree model is sufficient). The\ntraining function now fits the model without early stopping, while still\nhandling categorical features and the log\u2011transform of the target. The script\nruns a 5\u2011fold cross\u2011validation, prints the overall RMSE, retrains on the full\ndata, and creates a `submission.csv` in the `./working` folder if a test file is\npresent. All required functions (`train`, `prepare_val`, `predict`, `run`) are\nprovided with the exact signatures.", "The bug arose because `LGBMRegressor.fit` does not accept a `verbose` argument,\ncausing a TypeError. We remove the unsupported `verbose` parameter from the fit\ncall and keep the categorical handling unchanged. The script now defines the\nrequired `train`, `prepare_val`, `predict`, and `run` functions, performs 5\u2011fold\ncross\u2011validation to compute and print the RMSE, retrains on the full dataset,\ngenerates predictions for the test set (if present), and writes them to\n`./working/submission.csv`. All functionality is contained in a single\nself\u2011contained Python file.", "The improvement is to apply a log1p transformation to the target variable before\ntraining, which often stabilizes variance and improves regression performance;\nthe model will be trained on the transformed target and predictions will be\nconverted back with expm1. This change is applied consistently in\ncross\u2011validation and when generating the final submission, and the resulting\nRMSE on the original scale is printed.", "The bug was caused by the `train` function expecting a Series while the required\nsignature supplies a one\u2011column DataFrame, leading to shape mismatches when\nfitting LightGBM. I updated `train` to accept a DataFrame, extract the target\ncolumn as a 1\u2011D array, and keep the log\u2011transform logic. The helper functions\nnow work with the corrected output, and a main block performs 5\u2011fold\ncross\u2011validation, prints the RMSE, retrains on the full data, and writes\npredictions to `./working/submission.csv` if a test file exists.", "We add a log\u2011transform of the target to reduce skewness, training the LightGBM\nmodel on `log1p(y)` and applying `expm1` to the predictions before evaluation\nand submission. This simple change often yields a lower RMSE while keeping the\nsame pipeline structure.", "We add a log\u2011transform of the target variable (`log1p`) before training the\nLightGBM model and then apply the inverse transform (`expm1`) to the\npredictions. This often reduces skewness in the target distribution and improves\nRMSE while keeping the same API. The `train` function now fits on the\ntransformed target and stores a flag, and `predict` returns the back\u2011transformed\npredictions. The cross\u2011validation loop evaluates the metric on the original\nscale.", "We add a log\u2011transform of the target (`log1p`) before training the LightGBM\nmodel and apply the inverse (`expm1`) to the predictions. This often reduces\nskewness in the headcount distribution and improves RMSE while keeping the same\nAPI. The `train` function now fits on the transformed target and stores the\nmodel; `predict` back\u2011transforms the output before returning it. The\ncross\u2011validation loop evaluates RMSE on the original scale and the final\nsubmission is generated from the back\u2011transformed predictions.", "We add a log\u2011transform of the target (`log1p`) before training the LightGBM\nmodel and apply the inverse transform (`expm1`) to the predictions. This often\nreduces skewness in the headcount distribution and improves RMSE while keeping\nthe same API. The `train` function now fits on the transformed target and stores\nthe model; `predict` returns the back\u2011transformed predictions. The 5\u2011fold\ncross\u2011validation evaluates RMSE on the original scale and the final model is\nused to generate a `submission.csv` if a test set exists.", "We add a log\u2011transform of the target (`log1p`) to reduce skewness and enable\nearly stopping on the validation fold, which often yields a lower RMSE. The\n`train` function now accepts optional validation data and uses\n`early_stopping_rounds`. After prediction we apply the inverse `expm1` transform\nbefore computing the metric and saving the final submission.", "The issue arose because the LightGBM\u202f`LGBMRegressor.fit` call used the\n`early_stopping_rounds` argument, which isn\u2019t supported in the installed\nLightGBM version. The fix removes early stopping and fits the model on the\nlog\u2011transformed target directly. The training, validation preparation, and\nprediction functions are kept unchanged apart from this change, and the script\nnow runs a 5\u2011fold CV, prints the RMSE, retrains on the full data, and writes\ntest predictions to\u202f`./working/submission.csv` if a test file exists.", "We add a log\u2011transform of the target (`log1p`) before training the LightGBM\nmodel and apply the inverse (`expm1`) to the predictions. This often reduces\nskewness in the headcount distribution and improves RMSE while keeping the same\nAPI. The `train` function now fits on the transformed target and stores a flag,\nand `predict` returns the back\u2011transformed predictions. The cross\u2011validation\nloop evaluates the metric on the original scale and the final model generates a\nsubmission file with back\u2011transformed predictions.", "The improvement adds a log1p transformation of the target variable before\ntraining the LightGBM model and applies the inverse expm1 transformation to the\npredictions, which reduces skewness and typically lowers RMSE. The `train`\nfunction now fits on `log1p(y)`, stores the model, and the `predict` function\nback\u2011transforms the output. The evaluation metric is computed on the original\nscale after back\u2011transformation.", "We will improve the model by applying a log\u20111p transformation to the target\nbefore training, which reduces skewness and often yields a lower RMSE. The\n`train` function will fit LightGBM on the transformed target and store a flag,\nwhile `predict` will apply the inverse `expm1` to the model output. All\nevaluations and the final submission are computed on the original scale.", "The model can be improved by applying a `log1p` transformation to the target\nvariable before training, which reduces skewness and often lowers RMSE. The\n`train` function will fit the LightGBM regressor on `log1p(y)` and store the\ntransformation flag, while `predict` will apply `expm1` to the model outputs to\nreturn predictions on the original scale. The cross\u2011validation metric is\ncomputed after back\u2011transforming the predictions, and the final submission also\ncontains the inverse\u2011transformed values.", "We will improve the model by applying a `log1p` transformation to the target\nvariable before training, which reduces skewness and typically lowers RMSE. The\n`train` function will fit the LightGBM regressor on the transformed target and\nstore the model together with the categorical column list. The `predict`\nfunction will apply the inverse `expm1` transformation to the model outputs so\nthat predictions are on the original scale. The evaluation metric is computed on\nthe original scale after back\u2011transforming the out\u2011of\u2011fold predictions.", "We add a log\u20111p transformation of the target variable before training, which\nreduces skewness and generally improves RMSE. The `train` function fits LightGBM\non `log1p(y)`, while `predict` applies `expm1` to the model output to return\npredictions on the original scale. The evaluation metric is computed on the\nback\u2011transformed predictions, and test predictions are saved after inverse\ntransformation.", "We will improve the model by applying a `log1p` transformation to the target\nvariable before training the LightGBM regressor, which reduces skewness and\ntypically lowers RMSE. The `train` function will fit on the transformed target\nand store the model, while `predict` will apply the inverse `expm1`\ntransformation to the outputs so that predictions are on the original scale.\nThis single change is easy to test and often yields a noticeable performance\nboost.", "We add a log\u20111p transformation of the target before training, which reduces\nskewness and usually improves RMSE. The `train` function now fits the model on\n`log1p(y)` and stores the model; `predict` applies `expm1` to the raw\npredictions to return values on the original scale. The cross\u2011validation loop\nevaluates RMSE after the inverse transform, and the final submission is\ngenerated from the back\u2011transformed predictions.", "The fix ensures categorical columns keep the same category levels between\ntraining and validation by storing the training categories and re\u2011applying them\nto the validation data. It also drops rows with missing target values before\ntraining. The script now follows the required function signatures, runs a 5\u2011fold\ncross\u2011validation to print the OOF RMSE, retrains on the full dataset, generates\npredictions for the test set (if present), and writes them to\n`./working/submission.csv`.", "We can boost performance by applying a log\u20111p transformation to the target\nbefore training, which reduces skewness and often lowers RMSE. The `train`\nfunction will fit the LightGBM model on `log1p(y)` and store a flag, while\n`predict` will apply `expm1` to the model outputs to return predictions on the\noriginal scale. The cross\u2011validation metric is then computed on these\nback\u2011transformed predictions, and the final submission also contains the\ninverse\u2011transformed values.", "The fix replaces LightGBM with scikit\u2011learn\u2019s GradientBoostingRegressor (which\nis always available) and handles categorical variables by one\u2011hot encoding. The\ntraining function stores the list of dummy columns so the validation and test\ndata can be aligned exactly, filling missing columns with zeros. The script now\nperforms a 5\u2011fold cross\u2011validation, prints the overall RMSE, retrains on the\nfull data, and writes predictions for any provided test set to\n`./working/submission.csv`. All required functions (`train`, `prepare_val`,\n`predict`, `run`) follow the specified signatures.", "We will improve the model by applying a `log1p` transformation to the target\nvariable before training the LightGBM regressor, which reduces skewness and\ntypically lowers RMSE. The `train` function will fit the model on the\ntransformed target and store a flag indicating the transformation. The `predict`\nfunction will apply the inverse `expm1` transformation to the model outputs so\nthat predictions are on the original scale, and the evaluation metric will be\ncomputed on this original scale.", "The fix aligns the implementation with the required function signatures and\nensures the script can be executed directly: we keep the LightGBM model with\nlog\u2011target transformation, correctly handle categorical columns, and add a\n`__main__` block that runs 5\u2011fold cross\u2011validation, prints the OOF RMSE,\nretrains on the full data, generates test predictions (if a test file exists),\nand writes them to `./working/submission.csv`. All helper logic stays inside the\nmandated `train`, `prepare_val`, `predict`, and `run` functions.", "We add a `log1p` transformation of the target before training, which reduces\nskewness and typically lowers RMSE. The `train` function fits the LightGBM model\non `log1p(y)`, and the `predict` function applies `expm1` to the model output to\nreturn predictions on the original scale. The evaluation metric is computed on\nthe back\u2011transformed predictions, and the final submission is also\nback\u2011transformed.", "We will improve the model by applying a `log1p` transformation to the target\nvariable before training the LightGBM regressor, then applying the inverse\n`expm1` transformation to the predictions. This often reduces skewness in the\nheadcount distribution and yields a lower RMSE while keeping the same pipeline\nstructure.", "The improvement adds a `log1p` transformation of the target variable before\ntraining the LightGBM model and applies the inverse `expm1` transformation to\nthe predictions. This reduces skewness in the headcount distribution and\ntypically lowers the RMSE while keeping the same API and pipeline structure.", "The fix aligns the function signatures with the required API, removes unused\nparameters, and ensures categorical handling is consistent across training and\nvalidation. The `train` function now only accepts `X_train` and `y_train`,\nperforms a log\u2011transform on the target, fits a LightGBM regressor, and returns a\ndictionary containing the model, categorical columns, and a flag for\nlog\u2011scaling. `prepare_val` simply aligns validation data types to the training\ncategories, and `predict` back\u2011transforms predictions when needed. A main block\nperforms 5\u2011fold cross\u2011validation, prints the RMSE, retrains on the full dataset,\ngenerates test predictions if a test file exists, and writes them to\n`./working/submission.csv`.", "We add a log\u20111p transformation of the target variable before training the\nLightGBM model and apply the inverse `expm1` transformation to the predictions.\nThis often reduces skewness in the headcount distribution and improves RMSE\nwhile keeping the same pipeline structure.", "We add a log\u20111p transformation of the target variable before training the\nLightGBM model and apply the inverse `expm1` transformation to the predictions.\nThis simple change reduces target skewness and typically lowers the RMSE while\nkeeping the same pipeline structure.", "We add a log\u20111p transformation of the target before training the LightGBM model\nand apply the inverse `expm1` to the predictions. This reduces skewness in the\nheadcount distribution and usually lowers RMSE while keeping the same pipeline\nstructure. The `train` function now fits on `log1p(y)` and stores the model\ntogether with the categorical columns. The `predict` function back\u2011transforms\nthe model output, and the validation metric is computed on the original scale.", "We will improve the model by applying a `log1p` transformation to the target\nvariable before training the LightGBM regressor and then applying the inverse\n`expm1` transformation to the predictions. This reduces skewness in the\nheadcount distribution and typically lowers the RMSE. The `train` function will\nfit on the transformed target and store a flag, while `predict` will\nback\u2011transform the outputs. The evaluation metric is computed on the original\nscale after back\u2011transformation.", "We will improve the model by applying a `log1p` transformation to the target\nvariable before training the LightGBM regressor and then applying the inverse\n`expm1` transformation to the predictions. This reduces skewness in the\nheadcount distribution and typically lowers the RMSE while keeping the same\npipeline structure.", "The fix ensures the required function signatures match exactly, correctly aligns\ncategorical levels between training and validation/test data, and adds a main\nexecution block that performs 5\u2011fold cross\u2011validation, prints the OOF RMSE,\nretrains on the full training set, generates predictions for the test file (if\npresent), and writes them to `./working/submission.csv`. All steps are contained\nin a single script with the mandated `train`, `prepare_val`, `predict`, and\n`run` functions.", "We will improve the model by applying a log\u20111p transformation to the target\nvariable before training the LightGBM regressor and then applying the inverse\nexpm1 transformation to the predictions. This reduces target skewness and\ntypically yields a lower RMSE. The train function will store the transformation\nflag and the predict function will back\u2011transform when needed. The script will\nstill perform 5\u2011fold CV and save test predictions.", "We will enhance the model by applying a `log1p` transformation to the target\nvariable before training the LightGBM regressor, which reduces skewness and\ntypically lowers RMSE. The `train` function will fit the model on the\ntransformed target and store a flag indicating the transformation. The `predict`\nfunction will apply the inverse `expm1` to the raw predictions when the flag is\nset, ensuring outputs are on the original scale. This change is simple yet often\nyields a noticeable performance boost, and we will evaluate it using 5\u2011fold\ncross\u2011validation and save test predictions if a test set is present.", "The improvement adds a log1p transformation of the target during training and\napplies the inverse expm1 to predictions, which reduces target skewness and\noften lowers RMSE. The `train` function now fits the model on `log1p(y)` and\nstores a flag, while `predict` back\u2011transforms when the flag is set. The script\nruns 5\u2011fold CV, prints the OOF RMSE on the original scale, retrains on all data,\nand writes test predictions to `./working/submission.csv` if a test file exists.", "The bug stemmed from the `train` function having extra optional parameters that\ndo not match the required signature, which could cause the evaluation harness to\nreject the submission. I will rewrite the four required functions (`train`,\n`prepare_val`, `predict`, `run`) to exactly follow the specified signatures,\nkeep the LightGBM model with log\u2011target transformation, and ensure categorical\ncolumns are consistently encoded. The script will also perform a 5\u2011fold\ncross\u2011validation, print the RMSE, retrain on the full data, generate predictions\nfor the test set (if present), and save them to `./working/submission.csv`.", "Add a log\u20111p transformation of the target variable before model training and\napply the inverse `expm1` transformation to the predictions. This reduces\nskewness of the headcount distribution and often improves RMSE. The `train`\nfunction now fits on the transformed target and stores a flag, while `predict`\nback\u2011transforms when needed. The script runs 5\u2011fold CV, prints the OOF RMSE,\nretrains on the full data, and writes test predictions to\n`./working/submission.csv`.", "Add a log\u20111p transformation of the target before training the LightGBM model and\napply the inverse `expm1` transformation to the predictions during evaluation\nand submission generation. This simple scaling often reduces skewness and\nimproves RMSE. The `train` function will now fit on `log1p(y)` and store a flag,\nwhile `predict` will back\u2011transform when the flag is set. The rest of the\npipeline (categorical handling, 5\u2011fold CV, full\u2011data retraining, and test\nsubmission) remains unchanged.", "Apply a log1p transformation to the target before model training and\nback\u2011transform predictions with expm1 during evaluation and submission. This\nscaling often reduces skewness and can improve RMSE for headcount prediction.\nThe `train` function will store a flag indicating the transformation, and\n`predict` will apply the inverse transform when needed. The script will still\nperform 5\u2011fold CV and print the OOF RMSE.", "The fix aligns the implementation with the required function signatures, removes\nthe unused extra arguments from `train`, and adds logic inside the script\u2019s main\nblock to compute and print the RMSE on a hold\u2011out validation split. After\nevaluation, the model is retrained on the full training set and predictions for\nthe provided test file are saved to `./working/submission.csv`. All required\nfunctions (`train`, `prepare_val`, `predict`, `run`) are kept simple wrappers\naround the LightGBM model, and the script is self\u2011contained.", "We will enhance the model by applying a `log1p` transformation to the target\nvariable before training the LightGBM regressor and then applying the inverse\n`expm1` transformation to the predictions. This scaling often reduces target\nskewness and can improve RMSE. The `train` function will store a flag indicating\nthe transformation, and `predict` will back\u2011transform when needed. The script\nwill perform 5\u2011fold cross\u2011validation, print the OOF RMSE, retrain on the full\ndata, and save test predictions to `./working/submission.csv` if a test file\nexists.", "The fix aligns the functions with the required signatures, removes the unused\nextra arguments, and adds a main execution block that performs 5\u2011fold\ncross\u2011validation, prints the RMSE, retrains on the full training set, generates\npredictions for the test file (if present), and saves them to\n`./working/submission.csv`. Categorical columns are consistently encoded as\npandas \u201ccategory\u201d dtype, and the target is log\u2011transformed during training and\nback\u2011transformed for evaluation and final predictions.", "Add a log\u20111p transformation of the target variable before fitting the LightGBM\nmodel and apply the inverse `expm1` transformation to the predictions during\nevaluation and submission generation. This scaling reduces target skewness and\noften improves RMSE. The `train` function will store a flag indicating the\ntransformation, and `predict` will back\u2011transform when the flag is set. The\nscript runs 5\u2011fold CV, prints the OOF RMSE, retrains on the full data, and saves\ntest predictions to `./working/submission.csv` if a test file exists.", "The fix replaces the unsupported `early_stopping_rounds` argument with\nLightGBM\u2019s callback API, ensuring early stopping works across versions. The\ntraining function now logs\u2011transforms the target, converts categorical columns\nto the \u201ccategory\u201d dtype, and fits the model with appropriate callbacks.\nValidation data is prepared by aligning categorical columns, and predictions are\nback\u2011transformed from the log scale. The script runs a 5\u2011fold cross\u2011validation\nto report RMSE, retrains on the full dataset, and writes test predictions to\n`./working/submission.csv` if a test file is present.", "Add a log\u20111p transformation of the target variable before fitting the LightGBM\nmodel and apply the inverse `expm1` to the predictions, which often reduces\nskewness and improves RMSE. The `train` function will now fit on `np.log1p(y)`\nand store a flag indicating the transformation. The `predict` function will\nback\u2011transform the raw predictions when this flag is set. All other steps\n(categorical handling, K\u2011fold split, submission) remain unchanged, and the\nscript prints the 5\u2011fold CV OOF RMSE.", "The issue stems from mismatched function signatures and extra parameters in the\nprevious code, which prevent the evaluation harness from calling the required\n`train`, `prepare_val`, `predict`, and `run` functions correctly. I will align\nthe signatures exactly as specified, ensure `y_train` is accepted as a DataFrame\n(converting it to a 1\u2011D array internally), and keep the LightGBM model with\nlog\u2011target handling. The script will perform 5\u2011fold cross\u2011validation, print the\nOOF RMSE, retrain on the full data, and write test predictions to\n`./working/submission.csv` when a test file is present.", "We can improve the model by applying a log\u20111p transformation to the target\nvariable during training, which often reduces skewness and yields lower RMSE.\nThe `train` function will fit on `log1p(y)` and store a flag indicating the\ntransformation. The `predict` function will then apply the inverse `expm1` to\nthe raw predictions, ensuring the evaluation metric is computed on the original\nscale. This change is lightweight and keeps memory usage low.", "Add early stopping via LightGBM\u2019s callback API to each fold, using a validation\nsplit for stopping, which reduces over\u2011fitting and training time while often\nimproving RMSE. Keep the categorical handling unchanged and retain the original\nfeature set. After cross\u2011validation, compute and print the OOF RMSE, then\nretrain on the full data and write test predictions to\n`./working/submission.csv`.", "The fix aligns the functions with the required signatures, ensures `y_train` is\nhandled whether it comes as a DataFrame or Series, and uses the same LightGBM\nmodel with proper categorical handling. The script now performs 5\u2011fold\ncross\u2011validation, prints the RMSE, retrains on the full data, generates\npredictions for the test set (if present), and saves them to\n`./working/submission.csv`. All helper steps are encapsulated in the mandated\n`train`, `prepare_val`, `predict`, and `run` functions.", "The improvement adds proper early stopping using LightGBM\u2019s callback API, which\ntrims the number of trees and can improve generalization, thereby lowering the\nOOF RMSE. The `train` function now accepts optional validation data and, when\nprovided, fits with an early\u2011stopping callback based on RMSE. The\ncross\u2011validation loop passes the validation split to `train`, and the rest of\nthe pipeline remains unchanged. This change keeps memory usage low while\npotentially boosting performance.", "Add early stopping via LightGBM\u2019s callback API by passing validation data to\n`train` when available, which will stop training once RMSE does not improve for\n100 rounds. This reduces over\u2011fitting and speeds up training, potentially\nlowering the OOF RMSE. The rest of the pipeline (categorical handling, 5\u2011fold\nCV, and submission generation) stays unchanged. The script now prints the 5\u2011fold\nCV OOF RMSE and writes `submission.csv` if a test set exists.", "The bug was caused by mismatched function signatures \u2013 the original `train`\nexpected a Series for `y_train` while the required interface passes a DataFrame,\nand the helper functions were not aligned with the exact signatures. I updated\n`train` to accept a DataFrame (extracting the target column internally), kept\nthe categorical handling, and returned a dictionary with the model and\ncategorical columns. The `prepare_val` and `predict` functions now strictly\nfollow the required signatures, and `run` simply ties them together. A main\nblock loads the data, creates a hold\u2011out split, calls `run` to get predictions,\nprints the RMSE, retrains on the full dataset, and writes test predictions to\n`./working/submission.csv` if a test file exists.", "Apply a log\u20111p transformation to the target variable during training and\nback\u2011transform the predictions with expm1 before evaluation and submission. This\nscaling often reduces skewness and can improve RMSE without increasing memory\nusage. The `train` function will fit on the transformed target and store a flag,\nwhile `predict` will apply the inverse transformation when the flag is set. The\nrest of the pipeline (categorical handling, hold\u2011out split, final retraining,\nand submission) remains unchanged.", "Apply a log1p transformation to the target during training and back\u2011transform\npredictions with expm1 before evaluation. This scaling often reduces skewness\nand can improve RMSE without increasing memory usage. The `train` function will\nfit the model on the transformed target and store a flag indicating the\ntransformation. The `predict` function will apply the inverse transformation\nwhen the flag is set, ensuring outputs are on the original scale.", "We will improve the model by applying a log1p transformation to the target\nvariable during training, which often reduces skewness and improves regression\nperformance. The `train` function will fit the LightGBM model on the transformed\ntarget, and the `predict` function will back\u2011transform predictions with `expm1`\nbefore returning them. The hold\u2011out RMSE will be computed on the original scale\nto reflect true performance. This change adds minimal overhead and keeps memory\nusage low.", "The issue stems from categorical handling: unseen categories in the validation\nset cause LightGBM to error because the model expects the same category levels\nit saw during training. To fix this, we store each column\u2019s category levels when\ntraining and explicitly align the validation data to those levels before\nprediction. Additionally, we evaluate the model with 5\u2011fold cross\u2011validation\n(reporting the mean RMSE) and generate a `submission.csv` for the test set using\nthe full\u2011data model.", "The improvement adds early stopping using LightGBM\u2019s callback API, which trims\nunnecessary trees and can improve generalization, leading to a lower hold\u2011out\nRMSE while keeping memory usage modest. The `train` function now optionally\nreceives validation data and fits with an early\u2011stopping callback (stopping\nafter 100 rounds without improvement). The rest of the pipeline remains\nunchanged, and the script still outputs a submission file if test data is\npresent.", "The improvement adds a log1p transformation of the target during training and\nback\u2011transforms predictions with expm1, which often reduces skewness and\nimproves RMSE without increasing memory usage. The `train` function now fits on\nthe transformed target and stores a flag, while `predict` applies the inverse\ntransformation when the flag is set. All other steps remain the same, and the\nscript prints the hold\u2011out RMSE and creates a submission file if test data is\npresent.", "The improvement is to apply a log1p transformation to the target during training\nand back\u2011transform predictions with expm1, which often reduces skewness and can\nlower RMSE. The `train` function will fit the model on `log1p(y)` and store a\nflag, while `predict` will apply `expm1` to the raw predictions before returning\nthem. All other steps (categorical handling, hold\u2011out split, final retraining,\nand submission) remain unchanged. This change is lightweight in memory and\ncomputational cost.", "The improvement adds a log1p transformation of the target during training and\nback\u2011transforms predictions with expm1, which often reduces skewness and can\nlower RMSE. The `train` function now fits on `np.log1p(y)` and stores a flag,\nwhile `predict` applies `np.expm1` to the raw predictions before returning them.\nThe script performs 5\u2011fold cross\u2011validation, prints the mean RMSE across folds,\nretrains on the full data, and writes test predictions to\n`./working/submission.csv` if a test file exists.", "The issue arose because the LightGBM\u202fsklearn API version used does not accept\nthe `early_stopping_rounds` argument in `fit`. We remove early\u2011stopping and\nsimply train the model on the log\u2011transformed target, keeping the categorical\nhandling. The required `train`, `prepare_val`, `predict`, and `run` functions\nare implemented with the exact signatures, and a small script loads the data,\ncreates a hold\u2011out split, evaluates RMSE, prints it, then fits on all training\ndata and writes predictions for a test file (if present) to\n`./working/submission.csv`.", "The improvement adds LightGBM early stopping via its callback API, allowing the\nmodel to stop training when validation RMSE stops improving, which reduces\noverfitting and lowers the hold\u2011out error while also cutting memory usage by\nlimiting the number of trees. The `train` function now accepts optional\nvalidation data and uses `lgb.early_stopping` with a patience of 100 rounds. A\n5\u2011fold cross\u2011validation loop evaluates the mean RMSE on the validation folds and\nprints it. Finally, the model is retrained on the full training set and test\npredictions are saved to `./working/submission.csv` if test data exists."], "code": ["import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom typing import Any\n\n# LightGBM is available in the environment\nimport lightgbm as lgb\n\n\ndef _set_categorical(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas categorical dtype.\"\"\"\n    for col in cat_cols:\n        df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the training data.\n    Returns the fitted model and the list of categorical columns.\n    \"\"\"\n    # Identify categorical columns (object dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _set_categorical(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        n_estimators=500,\n        learning_rate=0.05,\n        num_leaves=31,\n        objective=\"regression\",\n        random_state=42,\n        n_jobs=-1,\n    )\n    model.fit(\n        X_train,\n        y_train.values.ravel(),\n        categorical_feature=cat_cols,\n        eval_metric=\"rmse\",\n    )\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Apply the same categorical conversion to the validation features.\n    Returns the processed validation DataFrame.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    X_val = _set_categorical(X_val.copy(), cat_cols)\n    return X_val\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for the processed validation data.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    End\u2011to\u2011end pipeline: train, prepare validation, predict.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    X_val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, X_val_prepared)\n\n\nif __name__ == \"__main__\":\n    # Load data\n    data_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(data_path)\n\n    # Target column\n    target_col = \"Unique Headcount\"\n    y = df[[target_col]]\n    X = df.drop(columns=[target_col])\n\n    # Simple train/validation split\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n\n    # Run the pipeline\n    val_preds = run(X_train, y_train, X_val)\n\n    # Compute RMSE\n    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n    print(f\"Validation RMSE: {rmse:.5f}\")\n\n    # If a test set existed we would generate predictions and save:\n    # test_path = os.path.join(\"input\", \"test.csv\")\n    # if os.path.exists(test_path):\n    #     test_df = pd.read_csv(test_path)\n    #     test_X = test_df.copy()\n    #     test_X_prepared = prepare_val(test_X, train_output)\n    #     test_preds = predict(train_output, test_X_prepared)\n    #     submission = pd.DataFrame({target_col: test_preds})\n    #     os.makedirs(\"working\", exist_ok=True)\n    #     submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\n# ----------------------------------------------------------------------\n# Required functions for the submission interface\n# ----------------------------------------------------------------------\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the training data.\n\n    Returns a dictionary containing the fitted model and the list of\n    categorical column names (used later for validation preprocessing).\n    \"\"\"\n    # Identify categorical columns (object dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n\n    # LightGBM can work directly with pandas categoricals\n    lgb_train = lgb.Dataset(\n        X_train,\n        label=y_train.squeeze(),\n        categorical_feature=cat_cols,\n        free_raw_data=False,\n    )\n\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.9,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 5,\n        \"seed\": 42,\n    }\n\n    model = lgb.train(params, lgb_train, num_boost_round=500, verbose_eval=False)\n\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation data categorical columns with the training data.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    # Ensure same dtype (category) for each categorical column\n    for col in cat_cols:\n        if col in X_val.columns:\n            X_val[col] = X_val[col].astype(\"category\")\n    return X_val\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for the processed validation (or test) data.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output, num_iteration=model.best_iteration)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint used by the evaluation script.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, val_prepared)\n\n\n# ----------------------------------------------------------------------\n# Main script: load data, split, train, evaluate, and (optionally) save predictions\n# ----------------------------------------------------------------------\nif __name__ == \"__main__\":\n    # Load the dataset\n    data_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(data_path)\n\n    # Target column\n    target_col = \"Unique Headcount\"\n\n    # Separate features and target\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # Convert all object columns to categorical dtype\n    for col in X.select_dtypes(include=[\"object\"]).columns:\n        X[col] = X[col].astype(\"category\")\n\n    # Train/validation split (80/20)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=df[\"Fiscal Year\"]\n    )\n\n    # Run the pipeline\n    val_preds = run(X_train, y_train, X_val)\n\n    # Compute RMSE\n    rmse = mean_squared_error(y_val, val_preds, squared=False)\n    print(f\"Validation RMSE: {rmse:.5f}\")\n\n    # ------------------------------------------------------------------\n    # If a separate test set existed we would generate predictions here\n    # and write them to ./working/submission.csv\n    # ------------------------------------------------------------------\n    # Example (commented out):\n    # test_path = os.path.join(\"input\", \"test.csv\")\n    # if os.path.exists(test_path):\n    #     test_df = pd.read_csv(test_path)\n    #     test_features = test_df.copy()\n    #     for col in test_features.select_dtypes(include=[\"object\"]).columns:\n    #         test_features[col] = test_features[col].astype(\"category\")\n    #     test_preds = run(X, y, test_features)  # retrain on full data if desired\n    #     submission = pd.DataFrame({\n    #         \"Id\": test_df[\"Id\"],  # adjust column name as needed\n    #         target_col: test_preds\n    #     })\n    #     os.makedirs(\"working\", exist_ok=True)\n    #     submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any, Tuple\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\n# -------------------------------------------------\n# Helper functions for categorical encoding\n# -------------------------------------------------\ndef factorize_df(df: pd.DataFrame, fit_dict: dict = None) -> Tuple[pd.DataFrame, dict]:\n    \"\"\"\n    Convert object columns to integer codes.\n    If fit_dict is provided, use it to transform; otherwise create new mappings.\n    Returns transformed dataframe and the mapping dictionary.\n    \"\"\"\n    df_enc = df.copy()\n    if fit_dict is None:\n        fit_dict = {}\n        for col in df_enc.select_dtypes(include=[\"object\", \"category\"]).columns:\n            df_enc[col], uniques = pd.factorize(df_enc[col], sort=True)\n            fit_dict[col] = uniques\n    else:\n        for col, uniques in fit_dict.items():\n            # map using the known uniques, unseen => -1\n            mapper = {cat: idx for idx, cat in enumerate(uniques)}\n            df_enc[col] = df_enc[col].map(mapper).fillna(-1).astype(int)\n    return df_enc, fit_dict\n\n\n# -------------------------------------------------\n# Required competition functions\n# -------------------------------------------------\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Returns a tuple (model, cat_mappings) where cat_mappings are the\n    factorization dictionaries for categorical columns.\n    \"\"\"\n    # Factorize categorical columns\n    X_enc, cat_maps = factorize_df(X_train)\n\n    # LightGBM dataset\n    lgb_train = lgb.Dataset(X_enc, label=y_train.values.ravel(), free_raw_data=False)\n\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"seed\": 42,\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.9,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 5,\n    }\n\n    model = lgb.train(params, lgb_train, num_boost_round=200)\n\n    return {\"model\": model, \"cat_maps\": cat_maps}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Apply the same factorization to validation features using the mappings\n    learned during training.\n    Returns the encoded validation dataframe.\n    \"\"\"\n    cat_maps = train_output[\"cat_maps\"]\n    X_val_enc, _ = factorize_df(X_val, fit_dict=cat_maps)\n    return X_val_enc\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.array:\n    \"\"\"\n    Generate predictions for the encoded validation data.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    return preds\n\n\ndef run(X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame) -> np.array:\n    \"\"\"\n    End\u2011to\u2011end pipeline: train \u2192 prepare validation \u2192 predict.\n    \"\"\"\n    train_out = train(X_train, y_train)\n    val_enc = prepare_val(X_val, train_out)\n    return predict(train_out, val_enc)\n\n\n# -------------------------------------------------\n# Main execution: load data, split, evaluate, and save predictions\n# -------------------------------------------------\nif __name__ == \"__main__\":\n    # Paths\n    INPUT_PATH = \"./input/train.csv\"\n    WORKING_DIR = \"./working\"\n    os.makedirs(WORKING_DIR, exist_ok=True)\n\n    # Load data\n    data = pd.read_csv(INPUT_PATH)\n\n    # Target column\n    TARGET_COL = \"Unique Headcount\"\n\n    # Separate features / target\n    X = data.drop(columns=[TARGET_COL])\n    y = data[[TARGET_COL]]\n\n    # Train/validation split (20% hold\u2011out)\n    X_tr, X_val, y_tr, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=data[\"Fiscal Year\"]\n    )\n\n    # Run the pipeline\n    val_preds = run(X_tr, y_tr, X_val)\n\n    # Compute RMSE\n    rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n    print(f\"Validation RMSE: {rmse:.5f}\")\n\n    # If a test set existed we would generate predictions here and save:\n    # test_path = \"./input/test.csv\"\n    # if os.path.exists(test_path):\n    #     test_df = pd.read_csv(test_path)\n    #     test_enc = prepare_val(test_df, train(X_tr, y_tr))\n    #     test_preds = predict(train_output, test_enc)\n    #     submission = pd.DataFrame({TARGET_COL: test_preds})\n    #     submission.to_csv(os.path.join(WORKING_DIR, \"submission.csv\"), index=False)", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any, Tuple, Dict, List\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\n# ----------------------------------------------------------------------\n# Helper functions for categorical encoding\n# ----------------------------------------------------------------------\ndef _factorize_df(\n    df: pd.DataFrame, cat_cols: List[str], mappings: Dict[str, pd.Series] = None\n) -> Tuple[pd.DataFrame, Dict[str, pd.Series]]:\n    \"\"\"\n    Convert categorical columns to integer codes.\n    If mappings are provided, use them (unseen categories become -1).\n    Returns transformed df and the mappings used/created.\n    \"\"\"\n    df_enc = df.copy()\n    new_mappings = {} if mappings is None else mappings\n\n    for col in cat_cols:\n        if mappings is None:\n            # create mapping from training data\n            codes, uniques = pd.factorize(df_enc[col], sort=True)\n            df_enc[col] = codes\n            # store mapping series: index is original category, value is code\n            mapping = pd.Series(data=np.arange(len(uniques)), index=uniques)\n            new_mappings[col] = mapping\n        else:\n            # use existing mapping, map unseen to -1\n            mapping = mappings[col]\n            df_enc[col] = df_enc[col].map(mapping).fillna(-1).astype(int)\n\n    return df_enc, new_mappings\n\n\n# ----------------------------------------------------------------------\n# Required interface functions\n# ----------------------------------------------------------------------\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Returns a dictionary containing the model, categorical column list,\n    and the factorization mappings.\n    \"\"\"\n    # Identify categorical columns (object dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n\n    # Factorize training data\n    X_enc, mappings = _factorize_df(X_train, cat_cols)\n\n    # LightGBM dataset\n    lgb_train = lgb.Dataset(\n        X_enc,\n        label=y_train.values.ravel(),\n        categorical_feature=cat_cols,\n        free_raw_data=False,\n    )\n\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.9,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 5,\n        \"seed\": 42,\n    }\n\n    model = lgb.train(params, lgb_train, num_boost_round=200)\n\n    return {\"model\": model, \"cat_cols\": cat_cols, \"mappings\": mappings}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Apply the same categorical encoding to validation data.\n    Returns the encoded validation dataframe.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    mappings = train_output[\"mappings\"]\n    X_enc, _ = _factorize_df(X_val, cat_cols, mappings)\n    return X_enc\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for the processed validation (or test) data.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output, num_iteration=model.best_iteration)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint used by the evaluator.\n    \"\"\"\n    train_out = train(X_train, y_train)\n    X_val_prepared = prepare_val(X_val, train_out)\n    return predict(train_out, X_val_prepared)\n\n\n# ----------------------------------------------------------------------\n# Main execution: train/validation split, metric reporting, optional test prediction\n# ----------------------------------------------------------------------\nif __name__ == \"__main__\":\n    # Load data\n    data_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(data_path)\n\n    # Target column\n    target_col = \"Unique Headcount\"\n    y = df[[target_col]]\n    X = df.drop(columns=[target_col])\n\n    # Train/validation split\n    X_tr, X_va, y_tr, y_va = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=df[\"Fiscal Year\"]\n    )\n\n    # Run the pipeline on the split\n    val_preds = run(X_tr, y_tr, X_va)\n\n    # Compute and print RMSE\n    rmse = mean_squared_error(y_va, val_preds, squared=False)\n    print(f\"Validation RMSE: {rmse:.5f}\")\n\n    # ------------------------------------------------------------------\n    # If a test set is present, train on full data and write submission\n    # ------------------------------------------------------------------\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        df_test = pd.read_csv(test_path)\n\n        # Train on full training data\n        full_out = train(X, y)\n\n        # Prepare test data\n        X_test_prepared = prepare_val(df_test, full_out)\n\n        # Predict\n        test_preds = predict(full_out, X_test_prepared)\n\n        # Save submission\n        submission = pd.DataFrame({\"Unique Headcount\": test_preds})\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any, Tuple\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\n# -------------------------------------------------\n# Core functions required by the competition API\n# -------------------------------------------------\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the training data.\n\n    Returns a tuple (model, cat_cols, cat_maps) where:\n        - model: trained LGBMRegressor\n        - cat_cols: list of categorical column names\n        - cat_maps: dict column -> list of categories observed in training\n    \"\"\"\n    # Identify categorical columns (object dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n\n    # Convert to pandas.Categorical with explicit categories\n    cat_maps = {}\n    for col in cat_cols:\n        X_train[col] = X_train[col].astype(\"category\")\n        cat_maps[col] = X_train[col].cat.categories.tolist()\n\n    # LightGBM expects raw numpy, but we can pass DataFrame directly\n    model = lgb.LGBMRegressor(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=31,\n        objective=\"regression\",\n        random_state=42,\n    )\n    model.fit(\n        X_train,\n        y_train.values.ravel(),\n        categorical_feature=cat_cols,\n        verbose=False,\n    )\n    return (model, cat_cols, cat_maps)\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation features with training categorical encodings.\n    Returns the processed X_val DataFrame.\n    \"\"\"\n    _, cat_cols, cat_maps = train_output\n    X_val = X_val.copy()\n    for col in cat_cols:\n        # Use training categories; unseen categories become NaN\n        cat_type = pd.CategoricalDtype(categories=cat_maps[col])\n        X_val[col] = X_val[col].astype(cat_type)\n    return X_val\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for the processed validation (or test) set.\n    \"\"\"\n    model, _, _ = train_output\n    X = prepare_val_output\n    preds = model.predict(X)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    End\u2011to\u2011end pipeline used by the script: train \u2192 prepare \u2192 predict.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, val_prepared)\n\n\n# -------------------------------------------------\n# Script execution: load data, evaluate, and predict test\n# -------------------------------------------------\ndef main():\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    submission_dir = \"working\"\n    os.makedirs(submission_dir, exist_ok=True)\n\n    # Load training data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # Hold\u2011out split for local evaluation\n    X_tr, X_val, y_tr, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=df[\"Fiscal Year\"]\n    )\n\n    # Run pipeline on validation split\n    val_preds = run(X_tr, y_tr, X_val)\n    rmse = mean_squared_error(y_val, val_preds, squared=False)\n    print(f\"Validation RMSE: {rmse:.5f}\")\n\n    # If a test set exists, train on full data and generate submission\n    if os.path.exists(test_path):\n        X_test = pd.read_csv(test_path)\n        # Train on full data\n        full_train_output = train(X, y)\n        X_test_prepared = prepare_val(X_test, full_train_output)\n        test_preds = predict(full_train_output, X_test_prepared)\n\n        # Save submission\n        submission_path = os.path.join(submission_dir, \"submission.csv\")\n        pd.DataFrame({target_col: test_preds}).to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the training data.\n\n    Returns a tuple (model, cat_cols, cat_maps) where:\n        - model: trained LGBMRegressor\n        - cat_cols: list of categorical column names\n        - cat_maps: dict column -> list of categories observed in training\n    \"\"\"\n    # Identify categorical columns (object or category dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n\n    # Store categories observed in training for each categorical column\n    cat_maps = {}\n    for col in cat_cols:\n        X_train[col] = X_train[col].astype(\"category\")\n        cat_maps[col] = X_train[col].cat.categories.tolist()\n\n    # Initialize LightGBM regressor\n    model = lgb.LGBMRegressor(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=31,\n        objective=\"regression\",\n        random_state=42,\n    )\n    # Fit without the unsupported verbose argument\n    model.fit(\n        X_train,\n        y_train.values.ravel(),\n        categorical_feature=cat_cols,\n    )\n    return (model, cat_cols, cat_maps)\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation features with training categorical encodings.\n    Returns the processed X_val DataFrame.\n    \"\"\"\n    _, cat_cols, cat_maps = train_output\n    X_val = X_val.copy()\n    for col in cat_cols:\n        # Use training categories; unseen categories become NaN\n        cat_type = pd.CategoricalDtype(categories=cat_maps[col])\n        X_val[col] = X_val[col].astype(cat_type)\n    return X_val\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for the processed validation (or test) set.\n    \"\"\"\n    model, _, _ = train_output\n    preds = model.predict(prepare_val_output)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint for submission.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, val_prepared)\n\n\ndef main():\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    working_dir = \"working\"\n    os.makedirs(working_dir, exist_ok=True)\n\n    # Load training data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # Hold\u2011out split for local evaluation\n    X_tr, X_val, y_tr, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=df[\"Fiscal Year\"]\n    )\n\n    # Run pipeline on validation split\n    val_preds = run(X_tr, y_tr, X_val)\n    rmse = mean_squared_error(y_val, val_preds, squared=False)\n    print(f\"Validation RMSE: {rmse:.5f}\")\n\n    # If a test set exists, train on full data and generate submission\n    if os.path.exists(test_path):\n        X_test = pd.read_csv(test_path)\n\n        # Train on full data\n        full_train_output = train(X, y)\n\n        # Prepare test data with same categorical mapping\n        X_test_prepared = prepare_val(X_test, full_train_output)\n\n        # Predict on test set\n        test_preds = predict(full_train_output, X_test_prepared)\n\n        # Save submission\n        submission_path = os.path.join(working_dir, \"submission.csv\")\n        pd.DataFrame({target_col: test_preds}).to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the training data.\n\n    Returns a dictionary with the fitted model and the list of categorical columns.\n    \"\"\"\n    # Identify categorical columns\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n\n    # Ensure categorical dtype for those columns\n    for col in cat_cols:\n        X_train[col] = X_train[col].astype(\"category\")\n\n    # LightGBM regressor (sklearn API)\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.1,\n        n_estimators=500,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(\n        X_train,\n        y_train.squeeze(),\n        categorical_feature=cat_cols,\n    )\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation data categorical columns with the training data.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    for col in cat_cols:\n        if col in X_val.columns:\n            X_val[col] = X_val[col].astype(\"category\")\n    return X_val\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for the processed validation (or test) data.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint for submission.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, val_prepared)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    data_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(data_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # Convert object columns to categorical dtype\n    for col in X.select_dtypes(include=[\"object\"]).columns:\n        X[col] = X[col].astype(\"category\")\n\n    # Train/validation split (80/20) with stratification on Fiscal Year\n    X_train, X_val, y_train, y_val = train_test_split(\n        X,\n        y,\n        test_size=0.2,\n        random_state=42,\n        stratify=df[\"Fiscal Year\"],\n    )\n\n    # Run pipeline and evaluate\n    val_preds = run(X_train, y_train, X_val)\n    rmse = mean_squared_error(y_val, val_preds, squared=False)\n    print(f\"Validation RMSE: {rmse:.5f}\")\n\n    # If a separate test set exists, generate predictions and save submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        for col in test_features.select_dtypes(include=[\"object\"]).columns:\n            test_features[col] = test_features[col].astype(\"category\")\n        test_preds = run(X, y, test_features)  # retrain on full data is optional\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Returns a dict with the fitted model and categorical column list.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=2000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(\n        X_train,\n        y_train.squeeze(),\n        categorical_feature=cat_cols,\n    )\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation (or test) data categorical columns with training data.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for processed validation or test data.\n    \"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepare_val_output)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint used in the original interface (fits on X_train/y_train and predicts on X_val).\n    \"\"\"\n    train_output = train(X_train, y_train)\n    val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, val_prepared)\n\n\nif __name__ == \"__main__\":\n    # Load data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # Identify categorical columns once (used for stratification)\n    strat_col = \"Fiscal Year\" if \"Fiscal Year\" in df.columns else None\n\n    # 5\u2011fold stratified CV (if possible)\n    n_splits = 5\n    if strat_col:\n        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        splits = skf.split(X, df[strat_col])\n    else:\n        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        splits = skf.split(X, y.squeeze())\n\n    oof_preds = np.zeros(len(df))\n    for train_idx, val_idx in splits:\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # categorical handling\n        cat_cols = X_tr.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n        X_tr = _ensure_category(X_tr, cat_cols)\n        X_va = _ensure_category(X_va, cat_cols)\n\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            metric=\"rmse\",\n            learning_rate=0.05,\n            n_estimators=2000,\n            num_leaves=31,\n            feature_fraction=0.9,\n            bagging_fraction=0.8,\n            bagging_freq=5,\n            random_state=42,\n            verbose=-1,\n        )\n        model.fit(\n            X_tr,\n            y_tr.squeeze(),\n            categorical_feature=cat_cols,\n            eval_set=[(X_va, y_va.squeeze())],\n            early_stopping_rounds=50,\n            verbose=False,\n        )\n        oof_preds[val_idx] = model.predict(X_va)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data for final submission\n    full_output = train(X, y)\n\n    # Test prediction if test file exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        # Align categories with training data\n        test_prepared = prepare_val(test_features, full_output)\n\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Returns a dict with the fitted model and categorical column list.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=2000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(\n        X_train,\n        y_train.squeeze(),\n        categorical_feature=cat_cols,\n    )\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation (or test) data categorical columns with training data.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for processed validation or test data.\n    \"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepare_val_output)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint for submission.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold\n        fold_output = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    If validation data is supplied, use early stopping.\n    Returns a dict with the fitted model and categorical column list.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    if X_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n\n    fit_params = {\n        \"categorical_feature\": cat_cols,\n    }\n\n    if X_val is not None and y_val is not None:\n        fit_params[\"eval_set\"] = [(X_val, y_val.squeeze())]\n        fit_params[\"early_stopping_rounds\"] = 100\n        fit_params[\"verbose\"] = False\n\n    model.fit(X_train, y_train.squeeze(), **fit_params)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation (or test) data categorical columns with training data.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for processed validation or test data.\n    \"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepare_val_output)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint for submission.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold with early stopping\n        fold_output = train(X_tr, y_tr, X_va, y_va)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data (no early stopping needed)\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Validation data is ignored (no early stopping) to avoid compatibility issues.\n    Returns a dict with the fitted model and categorical column list.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without early stopping\n    model.fit(X_train, y_train.squeeze(), categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data.\"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepare_val_output)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold (no early stopping)\n        fold_output = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor.\n    If validation data is provided, use early stopping.\n    Returns a dict with the fitted model and categorical column list.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=2000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n\n    if X_val is not None and y_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n        model.fit(\n            X_train,\n            y_train.squeeze(),\n            eval_set=[(X_val, y_val.squeeze())],\n            eval_metric=\"rmse\",\n            early_stopping_rounds=100,\n            categorical_feature=cat_cols,\n            verbose=False,\n        )\n    else:\n        model.fit(\n            X_train,\n            y_train.squeeze(),\n            categorical_feature=cat_cols,\n        )\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data.\"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepare_val_output)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation with internal early\u2011stopping\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # split training fold into inner train/val for early stopping\n        X_tr_inner, X_tr_val, y_tr_inner, y_tr_val = train_test_split(\n            X_tr, y_tr, test_size=0.1, random_state=42\n        )\n\n        # Train with early stopping\n        fold_output = train(X_tr_inner, y_tr_inner, X_tr_val, y_tr_val)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data (no early stopping needed)\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Returns a dict with the fitted model and the list of categorical columns.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=2000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without early stopping (compatible with all LightGBM versions)\n    model.fit(\n        X_train,\n        y_train.squeeze(),\n        categorical_feature=cat_cols,\n    )\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data.\"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepare_val_output)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold training data\n        fold_output = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict on validation data\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full training data\n    full_output = train(X, y)\n\n    # If test set exists, generate predictions and save submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Cast listed columns to pandas categorical dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor with early stopping on an internal hold\u2011out.\n    The target is log1p\u2011transformed; the transform parameters are stored for\n    inverse\u2011transforming predictions later.\n    \"\"\"\n    # Identify categorical columns\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log\u2011transform the target\n    y_log = np.log1p(y_train.squeeze())\n\n    # Split a small validation set for early stopping (10% of the fold)\n    X_tr, X_es, y_tr, y_es = train_test_split(\n        X_train, y_log, test_size=0.1, random_state=42\n    )\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit with early stopping\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_es, y_es)],\n        eval_metric=\"rmse\",\n        early_stopping_rounds=100,\n        categorical_feature=cat_cols,\n        verbose=False,\n    )\n    return {\n        \"model\": model,\n        \"cat_cols\": cat_cols,\n        \"target_shift\": 0.0,  # placeholder if needed later\n    }\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform the log\u2011scale target.\"\"\"\n    model = train_output[\"model\"]\n    pred_log = model.predict(prepare_val_output)\n    return np.expm1(pred_log)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for the competition.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_out = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Cast listed columns to pandas categorical dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    The target is log1p\u2011transformed; the transform parameters are stored\n    for inverse\u2011transforming predictions later.\n    \"\"\"\n    # Identify categorical columns\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log\u2011transform the target\n    y_log = np.log1p(y_train.squeeze())\n\n    # Simple train/validation split only for monitoring (no early stopping)\n    X_tr, X_es, y_tr, y_es = train_test_split(\n        X_train, y_log, test_size=0.1, random_state=42\n    )\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=2000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without early stopping (early stopping not supported in this env)\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_es, y_es)],\n        eval_metric=\"rmse\",\n        categorical_feature=cat_cols,\n        verbose=False,\n    )\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform the log\u2011scale target.\"\"\"\n    model = train_output[\"model\"]\n    pred_log = model.predict(prepare_val_output)\n    return np.expm1(pred_log)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint for submission.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_out = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Cast listed columns to pandas categorical dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    The target is log1p\u2011transformed; the transform parameters are stored\n    for inverse\u2011transforming predictions later.\n    \"\"\"\n    # Identify categorical columns\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log\u2011transform the target\n    y_log = np.log1p(y_train.squeeze())\n\n    # Simple train/validation split only for monitoring (no early stopping)\n    X_tr, X_es, y_tr, y_es = train_test_split(\n        X_train, y_log, test_size=0.1, random_state=42\n    )\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=2000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n    )\n    # Fit without unsupported verbose argument\n    model.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_es, y_es)],\n        eval_metric=\"rmse\",\n        categorical_feature=cat_cols,\n    )\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform the log\u2011scale target.\"\"\"\n    model = train_output[\"model\"]\n    pred_log = model.predict(prepare_val_output)\n    return np.expm1(pred_log)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint for submission.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    # Identify target column (assumed to be the one containing 'Headcount')\n    target_col = [c for c in df.columns if \"Headcount\" in c][0]\n\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_out = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame = None,\n    y_val: pd.Series = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the log\u2011transformed target.\n    Returns a dict with the fitted model and categorical column list.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without early stopping (using log1p target)\n    model.fit(X_train, np.log1p(y_train.values), categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions on original scale (expm1 of model output).\"\"\"\n    model = train_output[\"model\"]\n    log_pred = model.predict(prepare_val_output)\n    return np.expm1(log_pred)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train.squeeze())\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold (log\u2011target)\n        fold_output = train(X_tr, y_tr.squeeze())\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict on original scale\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE (original scale): {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y.squeeze())\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the log\u2011transformed target.\n    Returns a dict with the fitted model and categorical column list.\n    \"\"\"\n    # y_train is a one\u2011column DataFrame; convert to 1\u2011D array\n    y = y_train.iloc[:, 0].astype(float)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(\n        X_train,\n        np.log1p(y.values),\n        categorical_feature=cat_cols,\n    )\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions on original scale (expm1 of model output).\"\"\"\n    model = train_output[\"model\"]\n    log_pred = model.predict(prepare_val_output)\n    return np.expm1(log_pred)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation to report RMSE\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_out = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_prepared)\n\n    rmse = mean_squared_error(y.iloc[:, 0].astype(float), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE (original scale): {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # Predict on test set if present\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list, and target shift.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions on original scale (expm1 of log\u2011predictions).\"\"\"\n    model = train_output[\"model\"]\n    pred_log = model.predict(prepare_val_output)\n    return np.expm1(pred_log)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE (log\u2011target): {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    The target is log\u2011transformed to improve performance.\n    Returns a dict with the fitted model, categorical column list, and a flag.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data, applying inverse log\u2011transform.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    The target is log1p\u2011transformed to reduce skewness.\n    Returns a dict with the fitted model, categorical column list, and a flag.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform target\n    y_log = np.log1p(y_train.squeeze().values)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for processed validation or test data.\n    Returns predictions on the original target scale (expm1 of model output).\n    \"\"\"\n    model = train_output[\"model\"]\n    pred_log = model.predict(prepare_val_output)\n    return np.expm1(pred_log)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold (log\u2011target)\n        fold_output = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict (back\u2011transformed)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    The target is log\u2011transformed to reduce skewness.\n    Returns a dict with the fitted model, categorical column list and a flag.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data and inverse\u2011transform.\"\"\"\n    model = train_output[\"model\"]\n    pred_log = model.predict(prepare_val_output)\n    return np.expm1(pred_log)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame = None,\n    y_val: pd.Series = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Uses log1p target transformation and early stopping if validation data is supplied.\n    Returns a dict with the fitted model, categorical column list, and flag for log transform.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target\n    y_train_log = np.log1p(y_train)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n\n    if X_val is not None and y_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n        y_val_log = np.log1p(y_val)\n        model.fit(\n            X_train,\n            y_train_log,\n            eval_set=[(X_val, y_val_log)],\n            early_stopping_rounds=100,\n            categorical_feature=cat_cols,\n            verbose=False,\n        )\n    else:\n        model.fit(X_train, y_train_log, categorical_feature=cat_cols)\n\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if log target was used.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission (kept for compatibility).\"\"\"\n    train_output = train(X_train, y_train.squeeze())\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[target_col]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train with early stopping on the validation fold\n        fold_output = train(X_tr, y_tr, X_va, y_va)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict (inverse\u2011transformed inside predict)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y, oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data (no early stopping needed)\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Uses log1p target transformation.\n    Returns a dict with the fitted model, categorical column list, and flag for log transform.\n    \"\"\"\n    # Identify categorical columns (object or category dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log\u2011transform target\n    y_train_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without early stopping (compatible with all LightGBM versions)\n    model.fit(X_train, y_train_log, categorical_feature=cat_cols)\n\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if log target was used.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    prepared = prepare_val(X_val, train_output)\n    return predict(train_output, prepared)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    os.makedirs(\"working\", exist_ok=True)\n\n    # Load data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[target_col]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold\n        fold_out = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_out)\n\n        # Predict\n        oof_preds[val_idx] = predict(fold_out, X_va_prepared)\n\n    # Compute and print RMSE\n    rmse = mean_squared_error(y, oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # If test set exists, generate submission\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    The target is log1p\u2011transformed to reduce skewness.\n    Returns a dict with the fitted model, categorical column list, and a flag.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Convert y to 1\u2011D array and apply log1p\n    y_array = y_train.squeeze().values\n    y_log = np.log1p(y_array)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data, applying inverse log\u2011transform.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    # Inverse transform if target was log\u2011scaled\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold\n        fold_output = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict (already back\u2011transformed)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns dict with model, cat cols and a flag for log transform.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and back\u2011transform if target was log\u2011scaled.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical columns and a flag.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and back\u2011transform if target was log\u2011transformed.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold\n        fold_output = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list and a flag for log\u2011transform.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions on original scale (inverse log\u2011transform).\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    # inverse transform\n    return np.expm1(preds_log)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold (log\u2011target)\n        fold_output = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict (already back\u2011transformed)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list, and flag for log transform.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform the target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and back\u2011transform if target was log\u2011transformed.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold\n        fold_output = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list, and a flag.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without early stopping for compatibility\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions on original scale (inverse log transform).\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    return np.expm1(preds_log)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list, and a flag for log transform.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if log target was used.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list, and the\n    transformation flag.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if target was log\u2011scaled.\"\"\"\n    model = train_output[\"model\"]\n    raw_pred = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(raw_pred)\n    return raw_pred\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(\n    df: pd.DataFrame, cat_cols: list, cat_levels: dict = None\n) -> pd.DataFrame:\n    \"\"\"\n    Convert listed columns to pandas categorical dtype.\n    If cat_levels is provided, enforce the same categories as training.\n    \"\"\"\n    for col in cat_cols:\n        if col not in df.columns:\n            continue\n        if cat_levels and col in cat_levels:\n            df[col] = pd.Categorical(df[col], categories=cat_levels[col])\n        else:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list,\n    category levels mapping, and a flag for log\u2011target.\n    \"\"\"\n    # drop rows where target is NaN\n    mask = ~y_train.isna().squeeze()\n    X_train = X_train[mask].reset_index(drop=True)\n    y_train = y_train[mask].reset_index(drop=True)\n\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # store category levels for later alignment\n    cat_levels = {col: X_train[col].cat.categories.tolist() for col in cat_cols}\n\n    # log1p transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n\n    return {\n        \"model\": model,\n        \"cat_cols\": cat_cols,\n        \"cat_levels\": cat_levels,\n        \"log_target\": True,\n    }\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    cat_levels = train_output[\"cat_levels\"]\n    return _ensure_category(X_val.copy(), cat_cols, cat_levels)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if target was log\u2011scaled.\"\"\"\n    model = train_output[\"model\"]\n    raw_pred = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(raw_pred)\n    return raw_pred\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    prepared_val = prepare_val(X_val, train_output)\n    return predict(train_output, prepared_val)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    os.makedirs(\"working\", exist_ok=True)\n\n    # Load data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_out = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # Predict on test set if it exists\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list and a flag.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without early stopping for compatibility\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if target was log\u2011scaled.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\ndef _one_hot_encode(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Return df with categorical columns one\u2011hot encoded.\"\"\"\n    return pd.get_dummies(df, columns=cat_cols, drop_first=False)\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a GradientBoostingRegressor on the training data.\n    Returns a dict containing the fitted model, the dummy columns list,\n    and a flag indicating whether the target was log\u2011transformed.\n    \"\"\"\n    # Identify categorical columns (object or category dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    # One\u2011hot encode\n    X_enc = _one_hot_encode(X_train.copy(), cat_cols)\n\n    # Store column order for later alignment\n    dummy_cols = X_enc.columns.tolist()\n\n    # Log\u2011transform target to stabilize variance\n    y_log = np.log1p(y_train.squeeze())\n\n    model = GradientBoostingRegressor(\n        n_estimators=500,\n        learning_rate=0.05,\n        max_depth=4,\n        random_state=42,\n    )\n    model.fit(X_enc, y_log)\n\n    return {\n        \"model\": model,\n        \"dummy_cols\": dummy_cols,\n        \"cat_cols\": cat_cols,\n        \"log_target\": True,\n    }\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    One\u2011hot encode validation (or test) data using the same categorical columns\n    as in training and align its columns to the training dummy columns.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    X_enc = _one_hot_encode(X_val.copy(), cat_cols)\n\n    # Align to training columns, fill missing with 0\n    X_aligned = X_enc.reindex(columns=train_output[\"dummy_cols\"], fill_value=0)\n    return X_aligned\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions from the trained model and inverse\u2011transform\n    the log\u2011scale target if needed.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint used by the evaluator.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    X_val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, X_val_prepared)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    os.makedirs(\"working\", exist_ok=True)\n\n    # Load data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr = y.iloc[train_idx].reset_index(drop=True)\n\n        # Train on fold\n        fold_out = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # If test data exists, generate submission\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list, and a flag for log transform.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if log target was used.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas 'category' dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on a log\u2011transformed target.\n\n    Returns a dict containing the fitted model, list of categorical columns,\n    and a flag indicating that the target was log transformed.\n    \"\"\"\n    # Identify categorical columns (object or category dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log1p transform the target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if log target was used.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entry point used by the evaluator.\"\"\"\n    train_output = train(X_train, y_train)\n    prepared_val = prepare_val(X_val, train_output)\n    return predict(train_output, prepared_val)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    INPUT_DIR = \"input\"\n    WORKING_DIR = \"working\"\n    os.makedirs(WORKING_DIR, exist_ok=True)\n\n    # Load training data\n    train_path = os.path.join(INPUT_DIR, \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_out = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # If test set exists, generate predictions and save submission\n    test_path = os.path.join(INPUT_DIR, \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        submission_path = os.path.join(WORKING_DIR, \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list, and a flag indicating log transform.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions on original scale (inverse log transform).\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    # inverse transform\n    return np.expm1(preds_log)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold (log target)\n        fold_output = train(X_tr, y_tr)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict (back\u2011transformed)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list, and flag for log transform.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and apply inverse log transform if needed.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list and a flag.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and back\u2011transform if target was log\u2011scaled.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas 'category' dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on a log\u2011transformed target.\n\n    Returns a dict with the fitted model, list of categorical columns,\n    and a flag indicating that the target was log\u2011scaled.\n    \"\"\"\n    # Identify categorical columns\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log1p transform the target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation (or test) data categorical columns with training data.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions and back\u2011transform if target was log\u2011scaled.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint for submission: train on X_train/y_train and predict on X_val.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    prepared_val = prepare_val(X_val, train_output)\n    return predict(train_output, prepared_val)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    os.makedirs(\"working\", exist_ok=True)\n\n    # Load training data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation for evaluation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on the full training data\n    full_output = train(X, y)\n\n    # If test data exists, generate predictions and save submission\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list and a flag for log scaling.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target to reduce skew\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and back\u2011transform if target was log\u2011scaled.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_out = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_prepared)\n\n    rmse = mean_squared_error(df[target_col].values, oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # Generate submission if test data exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Applies log1p transformation to the target.\n    Returns a dict with the fitted model, categorical column list and a flag for log scaling.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data, applying inverse log transform.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    X_val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, X_val_prepared)\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit LightGBM on log\u2011transformed target.\n    Returns dict with model, categorical columns and a flag for log scaling.\n    \"\"\"\n    # Identify categorical columns\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation or test data categories with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: Any) -> np.ndarray:\n    \"\"\"\n    Predict with the trained model and back\u2011transform if log scaling was used.\n    \"\"\"\n    preds_log = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint used by the competition harness.\"\"\"\n    output = train(X_train, y_train)\n    return predict(output, prepare_val(X_val, output))\n\n\nif __name__ == \"__main__\":\n    # Load data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold CV\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        fold_out = train(X_tr, y_tr)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # Generate submission if test data exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical column list, and a flag for log scaling.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform\n    y_series = y_train.squeeze()\n    y_log = np.log1p(y_series)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without early stopping to keep compatibility\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and back\u2011transform if target was log\u2011scaled.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)  # inverse of log1p\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission (kept for compatibility).\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr = X.iloc[train_idx].reset_index(drop=True)\n        X_va = X.iloc[val_idx].reset_index(drop=True)\n        y_tr = y.iloc[train_idx].reset_index(drop=True)\n        y_va = y.iloc[val_idx].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # Generate submission if test data exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with the fitted model, categorical columns and a flag for log scaling.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log1p transform the target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if target was log\u2011scaled.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # Generate submission if test set exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _capture_categories(df: pd.DataFrame, cat_cols):\n    \"\"\"Store categories for each column.\"\"\"\n    cat_maps = {}\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n            cat_maps[col] = df[col].cat.categories\n    return cat_maps\n\n\ndef _apply_categories(df: pd.DataFrame, cat_maps):\n    \"\"\"Apply stored categories to a new dataframe.\"\"\"\n    for col, cats in cat_maps.items():\n        if col in df.columns:\n            df[col] = pd.Categorical(df[col], categories=cats)\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n    Returns a dict with model, categorical columns and category maps.\n    \"\"\"\n    # Identify categorical columns (object dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    # Capture categories to enforce same levels later\n    cat_maps = _capture_categories(X_train, cat_cols)\n\n    # Log1p transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n\n    return {\n        \"model\": model,\n        \"cat_cols\": cat_cols,\n        \"cat_maps\": cat_maps,\n        \"log_target\": True,\n    }\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation/test data categories with training data.\"\"\"\n    cat_maps = train_output[\"cat_maps\"]\n    return _apply_categories(X_val.copy(), cat_maps)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions and inverse\u2011transform if target was log\u2011scaled.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint used by the evaluator.\"\"\"\n    train_output = train(X_train, y_train)\n    val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, val_prepared)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    os.makedirs(\"working\", exist_ok=True)\n\n    # Load data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold CV\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_out = train(X_tr, y_tr)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # Predict on test set if exists\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n    # log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: Any) -> np.ndarray:\n    model = train_output[\"model\"]\n    preds = model.predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)  # inverse of log1p\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    out = train(X_train, y_train)\n    return predict(out, prepare_val(X_val, out))\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        fold_out = train(X_tr, y_tr)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y.squeeze(), oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # retrain on full data\n    full_out = train(X, y)\n\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_pre = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_pre)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)\n        print(f\"Test predictions saved to ./working/submission.csv\")\n\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n    y_log = np.log1p(y_train.squeeze())\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: Any) -> np.ndarray:\n    preds = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    out = train(X_train, y_train)\n    return predict(out, prepare_val(X_val, out))\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n        fold_out = train(X_tr, y_tr)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y.squeeze(), oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    full_out = train(X, y)\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_pre = prepare_val(test_df.copy(), full_out)\n        test_preds = predict(full_out, test_pre)\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)\n        print(f\"Test predictions saved to ./working/submission.csv\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"Fit LightGBM on training data with optional log\u2011transform of target.\"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Apply log1p transformation to target\n    y_log = np.log1p(y_train.squeeze())\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=3000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation or test data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions, applying inverse log transform if needed.\"\"\"\n    raw_pred = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(raw_pred)\n    return raw_pred\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_out = train(X_train, y_train)\n    return predict(train_out, prepare_val(X_val, train_out))\n\n\nif __name__ == \"__main__\":\n    # Load data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold CV\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        fold_out = train(X_tr, y_tr)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y.squeeze(), oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # Generate submission if test exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: Any) -> np.ndarray:\n    raw_pred = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(raw_pred)\n    return raw_pred\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    output = train(X_train, y_train)\n    return predict(output, prepare_val(X_val, output))\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        fold_out = train(X_tr, y_tr)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y.squeeze(), oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # retrain on full data\n    full_out = train(X, y)\n\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_pre = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_pre)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas 'category' dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Train a LightGBM regressor on the provided data.\n    The target is log\u2011transformed to improve stability.\n    Returns a dict containing the model, categorical column list and a flag for log\u2011target.\n    \"\"\"\n    # identify categorical columns\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation features with training preprocessing (categorical encoding).\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for the processed validation/test set.\n    Inverts the earlier log\u2011transform if applied.\n    \"\"\"\n    raw_pred = train_output[\"model\"].predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(raw_pred)\n    return raw_pred\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint used by the evaluation harness.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    # load data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        model_info = train(X_tr, y_tr)\n        X_va_pre = prepare_val(X_va, model_info)\n        oof_preds[val_idx] = predict(model_info, X_va_pre)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_model = train(X, y)\n\n    # Predict on test set if it exists\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_pre = prepare_val(test_features, full_model)\n        test_preds = predict(full_model, test_pre)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: Any) -> np.ndarray:\n    raw_pred = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(raw_pred)\n    return raw_pred\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    out = train(X_train, y_train)\n    return predict(out, prepare_val(X_val, out))\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold CV\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        fold_out = train(X_tr, y_tr)\n        X_va_prep = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_prep)\n\n    rmse = mean_squared_error(y.squeeze(), oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # train on full data\n    full_out = train(X, y)\n\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)\n        print(f\"Test predictions saved to ./working/submission.csv\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=3000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit on log\u2011scaled target\n    y_log = np.log1p(y_train.squeeze())\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: Any) -> np.ndarray:\n    raw_pred = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(raw_pred)\n    return raw_pred\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        fold_out = train(X_tr, y_tr)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_pre = prepare_val(test_df.copy(), full_out)\n        test_preds = predict(full_out, test_pre)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    return _ensure_category(X_val.copy(), train_output[\"cat_cols\"])\n\n\ndef predict(train_output: Any, prepared_X: Any) -> np.ndarray:\n    preds_log = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    out = train(X_train, y_train)\n    return predict(out, prepare_val(X_val, out))\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        fold_out = train(X_tr, y_tr)\n        X_va_prep = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_prep)\n\n    rmse = mean_squared_error(y.squeeze(), oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # retrain on full data\n    full_out = train(X, y)\n\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_feat = test_df.copy()\n        test_prep = prepare_val(test_feat, full_out)\n        test_pred = predict(full_out, test_prep)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_pred,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)\n        print(f\"Submission saved to ./working/submission.csv\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas 'category' dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Train a LightGBM regressor on the provided data.\n    Returns a dict containing the model, categorical column list and a flag\n    indicating that the target was log\u2011transformed.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target to stabilize variance\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Apply the same categorical conversion used during training.\n    \"\"\"\n    return _ensure_category(X_val.copy(), train_output[\"cat_cols\"])\n\n\ndef predict(train_output: Any, prepared_X: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions. If the target was log\u2011transformed during training,\n    the predictions are back\u2011transformed with expm1.\n    \"\"\"\n    preds_log = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint used by the evaluation harness.\n    Trains on X_train / y_train and returns predictions for X_val.\n    \"\"\"\n    model_info = train(X_train, y_train)\n    X_val_prepped = prepare_val(X_val, model_info)\n    return predict(model_info, X_val_prepped)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    os.makedirs(\"working\", exist_ok=True)\n\n    # Load data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # Hold\u2011out split for local evaluation\n    X_tr, X_val, y_tr, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42, shuffle=True\n    )\n\n    # Train on split and evaluate\n    model_info = train(X_tr, y_tr)\n    X_val_prepped = prepare_val(X_val, model_info)\n    val_pred = predict(model_info, X_val_prepped)\n    rmse = mean_squared_error(y_val.squeeze(), val_pred, squared=False)\n    print(f\"Hold\u2011out RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_model_info = train(X, y)\n\n    # If test data exists, generate submission\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepped = prepare_val(test_features, full_model_info)\n        test_pred = predict(full_model_info, test_prepped)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_pred,\n            }\n        )\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Returns a dict with the fitted model, categorical column list,\n    and a flag indicating whether the target was log\u2011scaled.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=3000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for processed validation or test data.\n    If the model was trained on a log\u2011scaled target, back\u2011transform the output.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # Generate submission if test data exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas categorical dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Returns a dict containing the model, categorical column list,\n    and a flag indicating whether the target was log\u2011scaled.\n    \"\"\"\n    # Identify categorical columns\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log\u2011transform the target to stabilize variance\n    y_log = np.log1p(y_train.squeeze())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=3000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation/test data categorical columns with those used in training.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for processed validation or test data.\n    Applies the inverse of the log\u2011transform if it was used during training.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint for the competition: train on X_train/y_train,\n    prepare X_val, and return predictions.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, val_prepared)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    os.makedirs(\"working\", exist_ok=True)\n\n    # Load training data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train on fold\n        fold_output = train(X_tr, y_tr)\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n        # Predict\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    # Compute and print RMSE on out\u2011of\u2011fold predictions\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on the full dataset\n    full_output = train(X, y)\n\n    # If test data exists, generate submission\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, Dict\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas 'category' dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame = None,\n    y_val: pd.Series = None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Fit LightGBM regressor on training data.\n    Returns a dict containing the model, categorical column list,\n    and a flag indicating that the target was log\u2011transformed.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log\u2011transform the target\n    y_log = np.log1p(y_train.values.ravel())\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit with early stopping on provided validation set if available\n    if X_val is not None and y_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n        y_val_log = np.log1p(y_val.values.ravel())\n        model.fit(\n            X_train,\n            y_log,\n            eval_set=[(X_val, y_val_log)],\n            eval_metric=\"rmse\",\n            early_stopping_rounds=200,\n            categorical_feature=cat_cols,\n            verbose=False,\n        )\n    else:\n        model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X: pd.DataFrame, train_output: Dict[str, Any]) -> pd.DataFrame:\n    \"\"\"Align categorical columns of validation or test data with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X.copy(), cat_cols)\n\n\ndef predict(train_output: Dict[str, Any], X_prepared: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions and back\u2011transform if target was log\u2011scaled.\"\"\"\n    preds_log = train_output[\"model\"].predict(X_prepared)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame) -> np.ndarray:\n    \"\"\"Entry point used for final submission predictions.\"\"\"\n    train_out = train(X_train, y_train)\n    X_val_pre = prepare_val(X_val, train_out)\n    return predict(train_out, X_val_pre)\n\n\nif __name__ == \"__main__\":\n    # Load data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target = \"Unique Headcount\"\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    # 5\u2011fold CV\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        fold_out = train(X_tr, y_tr, X_va, y_va)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y, oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # Generate submission if test data exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.drop(columns=[\"Id\"], errors=\"ignore\")\n        test_pre = prepare_val(test_features, full_out)\n        test_pred = predict(full_out, test_pre)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target: test_pred,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)\n        print(f\"Saved submission to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, Dict\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas 'category' dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Dict[str, Any]:\n    \"\"\"Fit LightGBM regressor on training data (log\u2011target).\"\"\"\n    # Identify categorical columns\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Log\u2011transform target\n    y_log = np.log1p(y_train.values.ravel())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without explicit validation (early stopping handled via callbacks later if needed)\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef train_with_val(\n    X_tr: pd.DataFrame,\n    y_tr: pd.DataFrame,\n    X_va: pd.DataFrame,\n    y_va: pd.DataFrame,\n) -> Dict[str, Any]:\n    \"\"\"Fit with validation using callbacks for early stopping.\"\"\"\n    cat_cols = X_tr.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_tr = _ensure_category(X_tr.copy(), cat_cols)\n    X_va = _ensure_category(X_va.copy(), cat_cols)\n\n    y_tr_log = np.log1p(y_tr.values.ravel())\n    y_va_log = np.log1p(y_va.values.ravel())\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    callbacks = [\n        lgb.early_stopping(stopping_rounds=200, verbose=False),\n        lgb.log_evaluation(period=0),\n    ]\n    model.fit(\n        X_tr,\n        y_tr_log,\n        eval_set=[(X_va, y_va_log)],\n        eval_metric=\"rmse\",\n        categorical_feature=cat_cols,\n        callbacks=callbacks,\n    )\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X: pd.DataFrame, train_output: Dict[str, Any]) -> pd.DataFrame:\n    \"\"\"Align categorical columns of validation/test data with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X.copy(), cat_cols)\n\n\ndef predict(train_output: Dict[str, Any], X_prepared: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions and back\u2011transform if target was log\u2011scaled.\"\"\"\n    preds_log = train_output[\"model\"].predict(X_prepared)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entry point used for final submission predictions.\"\"\"\n    train_out = train(X_train, y_train)\n    X_val_pre = prepare_val(X_val, train_out)\n    return predict(train_out, X_val_pre)\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]  # keep as DataFrame for compatibility\n\n    # 5\u2011fold cross\u2011validation\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        fold_out = train_with_val(X_tr, y_tr, X_va, y_va)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof_preds[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y.values.ravel(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # If test data exists, create submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        # Preserve Id column if present, otherwise create simple index\n        id_col = \"Id\" if \"Id\" in test_df.columns else None\n        test_features = test_df.drop(\n            columns=[id_col] if id_col else [], errors=\"ignore\"\n        )\n        test_pre = prepare_val(test_features, full_out)\n        test_pred = predict(full_out, test_pre)\n\n        submission = pd.DataFrame(\n            {\n                id_col: test_df[id_col] if id_col else np.arange(len(test_df)),\n                target_col: test_pred,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Saved submission to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame = None,\n    y_val: pd.Series = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Returns a dict with the fitted model, categorical column list,\n    and a flag indicating log\u2011target scaling.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # log\u2011transform target\n    y_log = np.log1p(y_train.values.ravel())\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=3000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data, back\u2011transforming if needed.\"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds)\n    return preds\n\n\ndef run(X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[target_col]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y, oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # Generate submission if test data exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas categorical dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on the provided training data.\n    Returns a dict with the fitted model, categorical column list,\n    and a flag indicating log\u2011target scaling.\n    \"\"\"\n    # Identify categorical columns (object or category dtype)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # Convert y_train to 1\u2011D array and apply log1p transformation\n    y_vals = np.ravel(y_train.values)\n    y_log = np.log1p(y_vals)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=3000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data, back\u2011transforming if needed.\"\"\"\n    model = train_output[\"model\"]\n    preds_log = model.predict(prepare_val_output)\n    if train_output.get(\"log_target\", False):\n        preds = np.expm1(preds_log)\n    else:\n        preds = preds_log\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission: train on X_train/y_train and predict on X_val.\"\"\"\n    train_output = train(X_train, y_train)\n    prepared_val = prepare_val(X_val, train_output)\n    return predict(train_output, prepared_val)\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]  # keep as DataFrame to match signature\n\n    # 5\u2011fold cross\u2011validation for OOF predictions\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        fold_output = train(X_tr, y_tr)\n        X_va_prepared = prepare_val(X_va, fold_output)\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(df[target_col].values, oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # Generate predictions for test set if it exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n    # log\u2011transform target\n    y_log = np.log1p(y_train.squeeze())\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y_log, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    raw_pred = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(raw_pred)\n    return raw_pred\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    train_out = train(X_train, y_train)\n    return predict(train_out, prepare_val(X_val, train_out))\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n        fold_out = train(X_tr, y_tr)\n        X_va_prep = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_prep)\n\n    rmse = mean_squared_error(y.squeeze(), oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # retrain on full data\n    full_out = train(X, y)\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_prepped = prepare_val(test_df.copy(), full_out)\n        test_pred = predict(full_out, test_prepped)\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_pred,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)\n        print(f\"Test predictions saved to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, Dict\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas categorical dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame = None,\n    y_val: pd.Series = None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Fit a LightGBM regressor.\n    If validation data is supplied, early stopping is applied via callbacks.\n    Returns a dict containing the fitted model and categorical column list.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    if X_val is not None and y_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n        callbacks = [lgb.early_stopping(stopping_rounds=100, verbose=False)]\n        model.fit(\n            X_train,\n            y_train,\n            categorical_feature=cat_cols,\n            eval_set=[(X_val, y_val)],\n            callbacks=callbacks,\n        )\n    else:\n        model.fit(X_train, y_train, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Dict[str, Any]) -> pd.DataFrame:\n    \"\"\"Align validation or test data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Dict[str, Any], prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data.\"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepared_X)\n\n\ndef run(X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame) -> np.ndarray:\n    \"\"\"Train on full data and predict on test set.\"\"\"\n    output = train(X_train, y_train)\n    prepared_test = prepare_val(X_test, output)\n    return predict(output, prepared_test)\n\n\nif __name__ == \"__main__\":\n    # Load data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[target_col]\n\n    # 5\u2011fold CV\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n        fold_out = train(X_tr, y_tr, X_va, y_va)\n        X_va_pre = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_pre)\n\n    rmse = mean_squared_error(y, oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # Generate submission if test exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.drop(columns=[\"Id\"], errors=\"ignore\")\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)\n        print(f\"Test predictions saved to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas categorical dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor and return model together with categorical column info.\n    y_train may be a DataFrame or Series.\n    \"\"\"\n    # flatten target\n    y = y_train.squeeze() if isinstance(y_train, pd.DataFrame) else y_train\n\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # No validation inside this function; early stopping handled externally if needed\n    model.fit(X_train, y, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation/test data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation/test data.\"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepare_val_output)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint used by the evaluator.\n    Trains on X_train/y_train and predicts on X_val.\n    \"\"\"\n    train_out = train(X_train, y_train)\n    X_val_pre = prepare_val(X_val, train_out)\n    return predict(train_out, X_val_pre)\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[target_col]\n\n    # 5\u2011fold cross\u2011validation\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n\n        # Train with early stopping on the validation fold\n        cat_cols = X_tr.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n        X_tr_enc = _ensure_category(X_tr.copy(), cat_cols)\n        X_va_enc = _ensure_category(X_va.copy(), cat_cols)\n\n        model = lgb.LGBMRegressor(\n            objective=\"regression\",\n            metric=\"rmse\",\n            learning_rate=0.05,\n            n_estimators=5000,\n            num_leaves=31,\n            feature_fraction=0.9,\n            bagging_fraction=0.8,\n            bagging_freq=5,\n            random_state=42,\n            verbose=-1,\n        )\n        model.fit(\n            X_tr_enc,\n            y_tr,\n            categorical_feature=cat_cols,\n            eval_set=[(X_va_enc, y_va)],\n            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n        )\n        oof_preds[val_idx] = model.predict(X_va_enc)\n\n    rmse = mean_squared_error(y, oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # Predict on test set if available\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        # Preserve Id if present, otherwise create one\n        id_col = \"Id\" if \"Id\" in test_df.columns else None\n        X_test = test_df.drop(columns=[id_col] if id_col else [], errors=\"ignore\")\n        X_test_pre = prepare_val(X_test, full_output)\n        test_preds = predict(full_output, X_test_pre)\n\n        submission = pd.DataFrame(\n            {\n                id_col: test_df[id_col] if id_col else np.arange(len(test_df)),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport pandas as pd\nimport numpy as np\nfrom typing import Any\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor. If validation data is provided, use early stopping.\n    Returns a dict with the fitted model and categorical column list.\n    \"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    if X_val is not None and y_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n        # use early stopping callback\n        callbacks = [lgb.early_stopping(stopping_rounds=100, verbose=False)]\n        model.fit(\n            X_train,\n            y_train.squeeze(),\n            eval_set=[(X_val, y_val.squeeze())],\n            eval_metric=\"rmse\",\n            categorical_feature=cat_cols,\n            callbacks=callbacks,\n        )\n    else:\n        model.fit(X_train, y_train.squeeze(), categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"Align validation (or test) data categorical columns with training data.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data.\"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepare_val_output)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint for submission.\"\"\"\n    train_output = train(X_train, y_train)\n    return predict(train_output, prepare_val(X_val, train_output))\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # 5\u2011fold cross\u2011validation\n    n_splits = 5\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    oof_preds = np.zeros(len(df))\n\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[train_idx].reset_index(drop=True), X.iloc[\n            val_idx\n        ].reset_index(drop=True)\n        y_tr, y_va = y.iloc[train_idx].reset_index(drop=True), y.iloc[\n            val_idx\n        ].reset_index(drop=True)\n\n        # Train with early stopping on validation fold\n        fold_output = train(X_tr, y_tr, X_va, y_va)\n\n        # Prepare validation data\n        X_va_prepared = prepare_val(X_va, fold_output)\n\n        # Predict\n        oof_preds[val_idx] = predict(fold_output, X_va_prepared)\n\n    rmse = mean_squared_error(y.squeeze(), oof_preds, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_output = train(X, y)\n\n    # If test set exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Test predictions saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, Dict\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.Series,\n    X_val: pd.DataFrame = None,\n    y_val: pd.Series = None,\n) -> Dict[str, Any]:\n    \"\"\"Fit LightGBM with optional early stopping using validation data.\"\"\"\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    if X_val is not None and y_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n        model.fit(\n            X_train,\n            y_train,\n            categorical_feature=cat_cols,\n            eval_set=[(X_val, y_val)],\n            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n        )\n    else:\n        model.fit(X_train, y_train, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Dict[str, Any]) -> pd.DataFrame:\n    \"\"\"Align validation or test categorical columns with training.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Dict[str, Any], prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation or test data.\"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepared_X)\n\n\ndef run(X_train: pd.DataFrame, y_train: pd.Series, X_val: pd.DataFrame) -> np.ndarray:\n    \"\"\"Entrypoint for submission predictions.\"\"\"\n    train_out = train(X_train, y_train)\n    return predict(train_out, prepare_val(X_val, train_out))\n\n\nif __name__ == \"__main__\":\n    # Load data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[target_col]\n\n    # 5\u2011fold CV\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(len(df))\n    for tr_idx, val_idx in kf.split(X):\n        X_tr, X_va = X.iloc[tr_idx].reset_index(drop=True), X.iloc[val_idx].reset_index(\n            drop=True\n        )\n        y_tr, y_va = y.iloc[tr_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(\n            drop=True\n        )\n        fold_out = train(X_tr, y_tr, X_va, y_va)\n        X_va_prep = prepare_val(X_va, fold_out)\n        oof[val_idx] = predict(fold_out, X_va_prep)\n\n    rmse = mean_squared_error(y, oof, squared=False)\n    print(f\"5\u2011Fold CV OOF RMSE: {rmse:.5f}\")\n\n    # Retrain on full data\n    full_out = train(X, y)\n\n    # Generate submission if test exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)\n        print(f\"Submission saved to ./working/submission.csv\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor.\n\n    Args:\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.DataFrame): Training target (single column).\n\n    Returns:\n        dict: Contains the fitted model and list of categorical columns.\n    \"\"\"\n    # Ensure y_train is a 1\u2011d array\n    y = y_train.squeeze()\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation features with training categorical columns.\n\n    Args:\n        X_val (pd.DataFrame): Validation features.\n        train_output (Any): Output from `train`.\n\n    Returns:\n        pd.DataFrame: Processed validation features.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions for processed validation/test data.\n\n    Args:\n        train_output (Any): Output from `train`.\n        prepare_val_output (Any): Processed features.\n\n    Returns:\n        np.ndarray: Predicted values.\n    \"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepare_val_output)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint for the submission: train on X_train/y_train and predict on X_val.\n\n    Returns:\n        np.ndarray: Predictions for X_val.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    prepared_X_val = prepare_val(X_val, train_output)\n    return predict(train_output, prepared_X_val)\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]  # keep as DataFrame to match signature\n\n    # Hold\u2011out split for evaluation\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Get predictions on validation set\n    val_preds = run(X_tr, y_tr, X_val)\n\n    # Compute and print RMSE\n    rmse = mean_squared_error(y_val.squeeze(), val_preds, squared=False)\n    print(f\"Hold\u2011out RMSE: {rmse:.5f}\")\n\n    # Retrain on full data for final submission\n    full_train_output = train(X, y)\n\n    # If test data exists, generate submission\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_train_output)\n        test_preds = predict(full_train_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    y = np.log1p(y_train.squeeze())  # log\u2011transform target\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    preds_log = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    train_output = train(X_train, y_train)\n    prepared_X_val = prepare_val(X_val, train_output)\n    return predict(train_output, prepared_X_val)\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    val_preds = run(X_tr, y_tr, X_val)\n    rmse = mean_squared_error(y_val.squeeze(), val_preds, squared=False)\n    print(f\"Hold\u2011out RMSE: {rmse:.5f}\")\n    full_train_output = train(X, y)\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_train_output)\n        test_preds = predict(full_train_output, test_prepared)\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"Fit LightGBM on log\u2011transformed target.\"\"\"\n    y = np.log1p(y_train.squeeze())  # log1p transform\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=3000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation/test features with training categorical columns.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Predict and back\u2011transform if target was log\u2011scaled.\"\"\"\n    preds_log = train_output[\"model\"].predict(prepared_X)\n    if train_output.get(\"log_target\", False):\n        return np.expm1(preds_log)\n    return preds_log\n\n\ndef run(X_tr: pd.DataFrame, y_tr: pd.DataFrame, X_val: pd.DataFrame) -> np.ndarray:\n    \"\"\"Train on provided split and predict on validation.\"\"\"\n    train_out = train(X_tr, y_tr)\n    X_val_pre = prepare_val(X_val, train_out)\n    return predict(train_out, X_val_pre)\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n    target = \"Unique Headcount\"\n    X = df.drop(columns=[target])\n    y = df[[target]]\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    val_pred = run(X_tr, y_tr, X_val)\n    rmse = mean_squared_error(y_val.squeeze(), val_pred, squared=False)\n    print(f\"Hold\u2011out RMSE: {rmse:.5f}\")\n\n    # Retrain on full data and create submission if test exists\n    full_out = train(X, y)\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_pred = predict(full_out, test_prepared)\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target: test_pred,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission.to_csv(os.path.join(\"working\", \"submission.csv\"), index=False)\n        print(\"Submission saved.\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit LightGBM on log\u2011transformed target.\n    Returns dict with model and categorical column list.\n    \"\"\"\n    y = np.log1p(y_train.squeeze())  # log1p transform\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=2000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Convert validation/test features to training categories.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Predict and back\u2011transform from log scale.\"\"\"\n    raw = train_output[\"model\"].predict(prepared_X)\n    return np.expm1(raw)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    train_out = train(X_train, y_train)\n    X_val_p = prepare_val(X_val, train_out)\n    return predict(train_out, X_val_p)\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]  # keep DataFrame for signature compatibility\n\n    # Hold\u2011out split for evaluation\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Validation predictions\n    val_pred = run(X_tr, y_tr, X_val)\n    rmse = mean_squared_error(y_val.squeeze(), val_pred, squared=False)\n    print(f\"Hold\u2011out RMSE: {rmse:.5f}\")\n\n    # Retrain on full data for final submission\n    full_out = train(X, y)\n\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_pred = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_pred,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, Dict, List\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: List[str]) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas categorical dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef _align_categories(\n    df: pd.DataFrame, cat_cols: List[str], cat_levels: Dict[str, List[Any]]\n) -> pd.DataFrame:\n    \"\"\"Force validation categories to have the same levels as training.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            # convert to category then set the known levels (unseen become NaN)\n            df[col] = df[col].astype(\"category\")\n            df[col] = df[col].cat.set_categories(cat_levels[col])\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit LightGBM on log\u2011transformed target.\n    Returns dict with model, categorical column list and category levels.\n    \"\"\"\n    y = np.log1p(y_train.squeeze())  # log1p transform\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    # store category levels for later alignment\n    cat_levels = {col: X_train[col].cat.categories.tolist() for col in cat_cols}\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=2000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"cat_levels\": cat_levels}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation features to training categories.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    cat_levels = train_output[\"cat_levels\"]\n    return _align_categories(X_val.copy(), cat_cols, cat_levels)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Predict on log\u2011scale and back\u2011transform.\"\"\"\n    raw = train_output[\"model\"].predict(prepared_X)\n    return np.expm1(raw)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Entrypoint used by the evaluation harness.\"\"\"\n    train_out = train(X_train, y_train)\n    X_val_prepared = prepare_val(X_val, train_out)\n    return predict(train_out, X_val_prepared)\n\n\nif __name__ == \"__main__\":\n    # paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    # load data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]  # keep DataFrame for signature compatibility\n\n    # 5\u2011fold cross\u2011validation\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    rmses = []\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        val_pred = run(X_tr, y_tr, X_val)\n        rmse = mean_squared_error(y_val.squeeze(), val_pred, squared=False)\n        rmses.append(rmse)\n    mean_rmse = np.mean(rmses)\n    print(f\"5\u2011fold CV Mean RMSE: {mean_rmse:.5f}\")\n\n    # Train on full data for final submission\n    full_out = train(X, y)\n\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_pred = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_pred,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, Dict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.Series = None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Fit a LightGBM regressor with optional early stopping.\n\n    Parameters\n    ----------\n    X_train : pd.DataFrame\n        Training features.\n    y_train : pd.DataFrame\n        Training target (single column).\n    X_val : pd.DataFrame, optional\n        Validation features for early stopping.\n    y_val : pd.Series, optional\n        Validation target for early stopping.\n\n    Returns\n    -------\n    dict\n        Contains the fitted model and list of categorical columns.\n    \"\"\"\n    y = y_train.squeeze()\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n\n    if X_val is not None and y_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n        model.fit(\n            X_train,\n            y,\n            categorical_feature=cat_cols,\n            eval_set=[(X_val, y_val)],\n            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n        )\n    else:\n        model.fit(X_train, y, categorical_feature=cat_cols)\n\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation/test features with training categorical columns.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation/test data.\"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepared_X)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Train on X_train/y_train and predict on X_val.\"\"\"\n    train_out = train(\n        X_train, y_train, X_val, None\n    )  # no early stopping in hold\u2011out run\n    prepared = prepare_val(X_val, train_out)\n    return predict(train_out, prepared)\n\n\nif __name__ == \"__main__\":\n    # Load data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]  # keep as DataFrame\n\n    # Hold\u2011out split for evaluation\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Train with early stopping on the hold\u2011out split\n    train_out = train(X_tr, y_tr, X_val, y_val.squeeze())\n    X_val_prepared = prepare_val(X_val, train_out)\n    val_preds = predict(train_out, X_val_prepared)\n\n    rmse = mean_squared_error(y_val.squeeze(), val_preds, squared=False)\n    print(f\"Hold\u2011out RMSE: {rmse:.5f}\")\n\n    # Retrain on full data for final submission\n    full_out = train(X, y)\n\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n\n    Args:\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.DataFrame): Training target (single column).\n\n    Returns:\n        dict: Contains the fitted model, list of categorical columns, and flag for log transform.\n    \"\"\"\n    y = np.log1p(y_train.squeeze())  # log1p transform\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=2000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation features with training categorical columns.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n    Generate predictions for processed validation/test data.\n\n    Args:\n        train_output (Any): Output from `train`.\n        prepared_X (pd.DataFrame): Processed features.\n\n    Returns:\n        np.ndarray: Predicted values on original target scale.\n    \"\"\"\n    model = train_output[\"model\"]\n    preds = model.predict(prepared_X)\n    if train_output.get(\"log_target\"):\n        preds = np.expm1(preds)  # back\u2011transform\n    return preds\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Train on X_train/y_train and predict on X_val.\"\"\"\n    train_output = train(X_train, y_train)\n    prepared_X_val = prepare_val(X_val, train_output)\n    return predict(train_output, prepared_X_val)\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]  # keep as DataFrame for signature compatibility\n\n    # Hold\u2011out split for evaluation\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Validation predictions\n    val_preds = run(X_tr, y_tr, X_val)\n\n    # Compute and print RMSE on original scale\n    rmse = mean_squared_error(y_val.squeeze(), val_preds, squared=False)\n    print(f\"Hold\u2011out RMSE: {rmse:.5f}\")\n\n    # Retrain on full data for final submission\n    full_train_output = train(X, y)\n\n    # Generate submission if test data exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_train_output)\n        test_preds = predict(full_train_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n\n    Args:\n        X_train (pd.DataFrame): Training features.\n        y_train (pd.DataFrame): Training target (single column).\n\n    Returns:\n        dict: Contains the fitted model, list of categorical columns, and a flag.\n    \"\"\"\n    y = np.log1p(y_train.squeeze())  # log1p transform\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=3000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    model.fit(X_train, y, categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols, \"log_target\": True}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation/test features with training categorical columns.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n    Generate predictions for processed validation/test data.\n\n    Args:\n        train_output (Any): Output from `train`.\n        prepared_X (pd.DataFrame): Processed features.\n\n    Returns:\n        np.ndarray: Predicted values on original scale.\n    \"\"\"\n    model = train_output[\"model\"]\n    raw_pred = model.predict(prepared_X)\n    # back\u2011transform if we used log target\n    if train_output.get(\"log_target\", False):\n        return np.expm1(raw_pred)\n    return raw_pred\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Train on X_train/y_train and predict on X_val.\"\"\"\n    train_output = train(X_train, y_train)\n    prepared_X_val = prepare_val(X_val, train_output)\n    return predict(train_output, prepared_X_val)\n\n\nif __name__ == \"__main__\":\n    # Load training data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]  # keep as DataFrame for signature compatibility\n\n    # Hold\u2011out split for evaluation\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Validation predictions\n    val_preds = run(X_tr, y_tr, X_val)\n\n    # Compute and print RMSE on original scale\n    rmse = mean_squared_error(y_val.squeeze(), val_preds, squared=False)\n    print(f\"Hold\u2011out RMSE: {rmse:.5f}\")\n\n    # Retrain on full data for final submission\n    full_train_output = train(X, y)\n\n    # Generate submission if test data exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_train_output)\n        test_preds = predict(full_train_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, Dict\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.Series = None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n\n    Parameters\n    ----------\n    X_train : pd.DataFrame\n        Training features.\n    y_train : pd.DataFrame\n        Single\u2011column target.\n    X_val : pd.DataFrame, optional\n        Validation features for early stopping.\n    y_val : pd.Series, optional\n        Validation target for early stopping.\n\n    Returns\n    -------\n    dict\n        Dictionary with fitted model and categorical column list.\n    \"\"\"\n    y = y_train.squeeze()\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    if X_val is not None and y_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n        model.fit(\n            X_train,\n            np.log1p(y),\n            eval_set=[(X_val, np.log1p(y_val))],\n            eval_metric=\"rmse\",\n            early_stopping_rounds=100,\n            categorical_feature=cat_cols,\n            verbose=False,\n        )\n    else:\n        model.fit(X_train, np.log1p(y), categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X: pd.DataFrame, train_output: Dict[str, Any]) -> pd.DataFrame:\n    \"\"\"Align validation/test features with training categorical columns.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X.copy(), cat_cols)\n\n\ndef predict(train_output: Dict[str, Any], X_prepared: pd.DataFrame) -> np.ndarray:\n    \"\"\"\n    Generate predictions and back\u2011transform from log scale.\n\n    Returns\n    -------\n    np.ndarray\n        Predicted target values on original scale.\n    \"\"\"\n    raw = train_output[\"model\"].predict(X_prepared)\n    return np.expm1(raw)\n\n\ndef run_cv(X: pd.DataFrame, y: pd.DataFrame, n_splits: int = 5) -> float:\n    \"\"\"Perform K\u2011fold CV and return mean RMSE.\"\"\"\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    rmses = []\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        train_out = train(X_tr, y_tr, X_val, y_val.squeeze())\n        X_val_prep = prepare_val(X_val, train_out)\n        preds = predict(train_out, X_val_prep)\n        rmse = mean_squared_error(y_val.squeeze(), preds, squared=False)\n        rmses.append(rmse)\n    return np.mean(rmses)\n\n\nif __name__ == \"__main__\":\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n    target = \"Unique Headcount\"\n    X = df.drop(columns=[target])\n    y = df[[target]]\n\n    mean_rmse = run_cv(X, y, n_splits=5)\n    print(f\"5\u2011fold CV Hold\u2011out RMSE: {mean_rmse:.5f}\")\n\n    # Train on full data and create submission if test exists\n    full_out = train(X, y)\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert listed columns to pandas 'category' dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any:\n    \"\"\"\n    Fit a LightGBM regressor on log\u2011transformed target.\n\n    Returns a dict containing the fitted model and categorical column list.\n    \"\"\"\n    # target as 1\u2011d array\n    y = y_train.squeeze()\n    # identify categorical columns (object or category)\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n    # Fit without early stopping (compatible with older LightGBM versions)\n    model.fit(X_train, np.log1p(y), categorical_feature=cat_cols)\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> Any:\n    \"\"\"\n    Align validation / test features with training categorical columns.\n    \"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepare_val_output: Any) -> np.ndarray:\n    \"\"\"\n    Generate predictions on the original scale (inverse of log1p).\n    \"\"\"\n    raw_pred = train_output[\"model\"].predict(prepare_val_output)\n    return np.expm1(raw_pred)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"\n    Entrypoint required by the benchmark.\n    \"\"\"\n    train_output = train(X_train, y_train)\n    X_val_prepared = prepare_val(X_val, train_output)\n    return predict(train_output, X_val_prepared)\n\n\nif __name__ == \"__main__\":\n    # Paths\n    train_path = os.path.join(\"input\", \"train.csv\")\n    test_path = os.path.join(\"input\", \"test.csv\")\n    # Load data\n    df = pd.read_csv(train_path)\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]\n\n    # Hold\u2011out split for evaluation\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    # Train & evaluate\n    val_preds = run(X_tr, y_tr, X_val)\n    rmse = mean_squared_error(y_val.squeeze(), val_preds, squared=False)\n    print(f\"Hold\u2011out RMSE: {rmse:.5f}\")\n\n    # Train on full data\n    full_output = train(X, y)\n\n    # If test data exists, create submission\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_output)\n        test_preds = predict(full_output, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")", "import os\nimport numpy as np\nimport pandas as pd\nfrom typing import Any, Dict\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n\ndef _ensure_category(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n    \"\"\"Convert specified columns to pandas category dtype.\"\"\"\n    for col in cat_cols:\n        if col in df.columns:\n            df[col] = df[col].astype(\"category\")\n    return df\n\n\ndef train(\n    X_train: pd.DataFrame,\n    y_train: pd.DataFrame,\n    X_val: pd.DataFrame = None,\n    y_val: pd.DataFrame = None,\n) -> Dict[str, Any]:\n    \"\"\"\n    Fit a LightGBM regressor, optionally with early stopping.\n\n    Parameters\n    ----------\n    X_train : pd.DataFrame\n        Training features.\n    y_train : pd.DataFrame\n        Training target (single column).\n    X_val : pd.DataFrame, optional\n        Validation features for early stopping.\n    y_val : pd.DataFrame, optional\n        Validation target for early stopping.\n\n    Returns\n    -------\n    dict\n        Dictionary containing the fitted model and list of categorical columns.\n    \"\"\"\n    y = y_train.squeeze()\n    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n    X_train = _ensure_category(X_train.copy(), cat_cols)\n\n    model = lgb.LGBMRegressor(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        n_estimators=5000,\n        num_leaves=31,\n        feature_fraction=0.9,\n        bagging_fraction=0.8,\n        bagging_freq=5,\n        random_state=42,\n        verbose=-1,\n    )\n\n    if X_val is not None and y_val is not None:\n        X_val = _ensure_category(X_val.copy(), cat_cols)\n        y_val_arr = y_val.squeeze()\n        model.fit(\n            X_train,\n            y,\n            categorical_feature=cat_cols,\n            eval_set=[(X_val, y_val_arr)],\n            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n        )\n    else:\n        model.fit(X_train, y, categorical_feature=cat_cols)\n\n    return {\"model\": model, \"cat_cols\": cat_cols}\n\n\ndef prepare_val(X_val: pd.DataFrame, train_output: Any) -> pd.DataFrame:\n    \"\"\"Align validation/test features with training categorical columns.\"\"\"\n    cat_cols = train_output[\"cat_cols\"]\n    return _ensure_category(X_val.copy(), cat_cols)\n\n\ndef predict(train_output: Any, prepared_X: pd.DataFrame) -> np.ndarray:\n    \"\"\"Generate predictions for processed validation/test data.\"\"\"\n    model = train_output[\"model\"]\n    return model.predict(prepared_X)\n\n\ndef run(\n    X_train: pd.DataFrame, y_train: pd.DataFrame, X_val: pd.DataFrame\n) -> np.ndarray:\n    \"\"\"Train on X_train/y_train and predict on X_val.\"\"\"\n    train_out = train(X_train, y_train)\n    X_val_prepared = prepare_val(X_val, train_out)\n    return predict(train_out, X_val_prepared)\n\n\nif __name__ == \"__main__\":\n    # Load data\n    train_path = os.path.join(\"input\", \"train.csv\")\n    df = pd.read_csv(train_path)\n\n    target_col = \"Unique Headcount\"\n    X = df.drop(columns=[target_col])\n    y = df[[target_col]]  # keep as DataFrame for signature compatibility\n\n    # 5\u2011fold cross\u2011validation\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    rmses = []\n    for train_idx, val_idx in kf.split(X):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        # Train with early stopping using validation split\n        train_out = train(X_tr, y_tr, X_val, y_val)\n        X_val_prepared = prepare_val(X_val, train_out)\n        val_pred = predict(train_out, X_val_prepared)\n\n        rmse = mean_squared_error(y_val.squeeze(), val_pred, squared=False)\n        rmses.append(rmse)\n\n    mean_rmse = np.mean(rmses)\n    print(f\"5\u2011fold CV Hold\u2011out RMSE: {mean_rmse:.5f}\")\n\n    # Retrain on full dataset\n    full_out = train(X, y)\n\n    # Generate submission if test set exists\n    test_path = os.path.join(\"input\", \"test.csv\")\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n        test_features = test_df.copy()\n        test_prepared = prepare_val(test_features, full_out)\n        test_preds = predict(full_out, test_prepared)\n\n        submission = pd.DataFrame(\n            {\n                \"Id\": (\n                    test_df[\"Id\"]\n                    if \"Id\" in test_df.columns\n                    else np.arange(len(test_df))\n                ),\n                target_col: test_preds,\n            }\n        )\n        os.makedirs(\"working\", exist_ok=True)\n        submission_path = os.path.join(\"working\", \"submission.csv\")\n        submission.to_csv(submission_path, index=False)\n        print(f\"Submission saved to {submission_path}\")"], "term_out": ["[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001258 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 201\n[LightGBM] [Info] Number of data points in the train set: 46239, number of used features: 13\n[LightGBM] [Info] Start training from score 8.363935\nValidation RMSE: 7.01148\nExecution time: a second seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 106, in <module>\n    val_preds = run(X_train, y_train, X_val)\n  File \"runfile.py\", line 76, in run\n    train_output = train(X_train, y_train)\n  File \"runfile.py\", line 44, in train\n    model = lgb.train(params, lgb_train, num_boost_round=500, verbose_eval=False)\nTypeError: train() got an unexpected keyword argument 'verbose_eval'\nExecution time: a moment seconds (time limit is an hour).", "Validation RMSE: 8.26484\nExecution time: 23 minutes seconds (time limit is an hour).", "Validation RMSE: 7.58011\nExecution time: 24 minutes seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 125, in <module>\n    main()\n  File \"runfile.py\", line 106, in main\n    val_preds = run(X_tr, y_tr, X_val)\n  File \"runfile.py\", line 79, in run\n    train_output = train(X_train, y_train)\n  File \"runfile.py\", line 40, in train\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'verbose'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001441 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 203\n[LightGBM] [Info] Number of data points in the train set: 46239, number of used features: 13\n[LightGBM] [Info] Start training from score 8.294903\nValidation RMSE: 7.04776\nExecution time: a second seconds (time limit is an hour).", "Validation RMSE: 6.87466\nExecution time: a second seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 120, in <module>\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.24504\nExecution time: 22 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 110, in <module>\n    fold_output = train(X_tr, y_tr, X_va, y_va)\n  File \"runfile.py\", line 57, in train\n    model.fit(X_train, y_train.squeeze(), **fit_params)\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 5.84365\nExecution time: a minute seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 113, in <module>\n    fold_output = train(X_tr_inner, y_tr_inner, X_tr_val, y_tr_val)\n  File \"runfile.py\", line 47, in train\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.24504\nExecution time: 22 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 108, in <module>\n    fold_out = train(X_tr, y_tr)\n  File \"runfile.py\", line 49, in train\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 105, in <module>\n    fold_out = train(X_tr, y_tr)\n  File \"runfile.py\", line 49, in train\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'verbose'\nExecution time: a moment seconds (time limit is an hour).", "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001315 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 201\n[LightGBM] [Info] Number of data points in the train set: 41615, number of used features: 13\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 1.580407\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[L\n ... [6504 characters truncated] ... \nreq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 1.577342\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n5\u2011Fold CV OOF RMSE: 7.03620\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001782 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 204\n[LightGBM] [Info] Number of data points in the train set: 52019, number of used features: 13\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Start training from score 1.579803\nExecution time: 27 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE (original scale): 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE (original scale): 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE (log\u2011target): 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 111, in <module>\n    fold_output = train(X_tr, y_tr, X_va, y_va)\n  File \"runfile.py\", line 51, in train\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 13.91387\nExecution time: 5 minutes seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\n5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: 2 minutes seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.70668\nExecution time: 37 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.70668\nExecution time: 38 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "Hold\u2011out RMSE: 5.94562\nExecution time: 22 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.70668\nExecution time: 42 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.70668\nExecution time: 38 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 105, in <module>\n    fold_out = train(X_tr, y_tr, X_va, y_va)\n  File \"runfile.py\", line 49, in train\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42129\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.70668\nExecution time: 37 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.70668\nExecution time: 37 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 6.42191\nExecution time: a minute seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 5.84906\nExecution time: 59 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 5.84906\nExecution time: 58 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 5.84906\nExecution time: 59 seconds seconds (time limit is an hour).", "5\u2011Fold CV OOF RMSE: 5.84906\nExecution time: a minute seconds (time limit is an hour).", "Hold\u2011out RMSE: 5.65920\nExecution time: 21 seconds seconds (time limit is an hour).", "Hold\u2011out RMSE: 5.94562\nExecution time: 21 seconds seconds (time limit is an hour).", "Hold\u2011out RMSE: 6.23414\nExecution time: 13 seconds seconds (time limit is an hour).", "Hold\u2011out RMSE: 6.53434\nExecution time: 9 seconds seconds (time limit is an hour).", "5\u2011fold CV Mean RMSE: 6.98797\nExecution time: 25 seconds seconds (time limit is an hour).", "Hold\u2011out RMSE: 5.69985\nExecution time: 18 seconds seconds (time limit is an hour).", "Hold\u2011out RMSE: 6.53434\nExecution time: 8 seconds seconds (time limit is an hour).", "Hold\u2011out RMSE: 6.23414\nExecution time: 12 seconds seconds (time limit is an hour).", "Traceback (most recent call last):\n  File \"runfile.py\", line 116, in <module>\n    mean_rmse = run_cv(X, y, n_splits=5)\n  File \"runfile.py\", line 101, in run_cv\n    train_out = train(X_tr, y_tr, X_val, y_val.squeeze())\n  File \"runfile.py\", line 61, in train\n    model.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: a moment seconds (time limit is an hour).", "Hold\u2011out RMSE: 5.94562\nExecution time: 21 seconds seconds (time limit is an hour).", "5\u2011fold CV Hold\u2011out RMSE: 5.84872\nExecution time: a minute seconds (time limit is an hour)."], "analysis": ["The script successfully trains a LightGBM regressor, processes categorical features, and evaluates on a validation split achieving an RMSE of approximately 7.01. No errors or bugs were detected.", "The code fails because LightGBM's `lgb.train` is called with an unsupported `verbose_eval` argument, causing a TypeError. Remove the `verbose_eval` parameter (or set it via callbacks) to fix the bug. After fixing, the script should run and report validation RMSE.", "The training pipeline executed successfully, fitting a LightGBM regressor and achieving a validation RMSE of approximately 8.26. All required functions are correctly implemented and return the expected outputs.", "The model achieved a validation RMSE of approximately 7.58 using LightGBM with categorical encoding. The pipeline successfully trains, validates, and generates predictions for a test set when available.", "The script crashes during model training because LGBMRegressor.fit is called with an unsupported 'verbose' argument, raising a TypeError. Removing the 'verbose' parameter (or replacing it with appropriate callbacks) will allow the model to train and the pipeline to produce validation RMSE and test predictions.", "The LightGBM model trained without errors and achieved a validation RMSE of approximately 7.05, indicating reasonable predictive performance. The pipeline correctly handles categorical features and produces test predictions saved to the working directory.", "The script runs without errors, training a LightGBM regressor and achieving a validation RMSE of approximately 6.87. All required functions are correctly implemented and return appropriate types.", "The script crashes because LGBMRegressor.fit is called with the argument early_stopping_rounds, which is not supported by the LightGBM version in the execution environment. Remove the early_stopping_rounds argument (or replace it with a callback compatible with the installed version) and optionally set n_estimators high enough; the rest of the pipeline works correctly.", "The script successfully trains a LightGBM regressor, performs 5\u2011fold cross\u2011validation, and achieves an out\u2011of\u2011fold RMSE of approximately 6.245. The functions adhere to the required signatures and the pipeline runs without errors.", "The training function passes 'early_stopping_rounds' directly to LGBMRegressor.fit, which raises a TypeError in the current LightGBM version. Replace this with the appropriate callback (lgb.early_stopping) and ensure eval_set is provided. After fixing, the script should run and report the OOF RMSE.", "The script runs successfully, training a LightGBM model with 5\u2011fold cross\u2011validation and achieving an out\u2011of\u2011fold RMSE of approximately 5.84. No errors or bugs were detected.", "The training script crashes because LGBMRegressor.fit is called with the argument early_stopping_rounds, which is not supported in the installed LightGBM version. Remove the early_stopping_rounds parameter (or switch to lgb.train with callbacks) and optionally wrap the call in a try/except to fall back to a plain fit. After fixing this, the script should run and report the CV RMSE and generate predictions.", "The training script runs successfully, fitting a LightGBM regressor and achieving an out-of-fold RMSE of approximately 6.245 on a 5\u2011fold cross\u2011validation. The implementation adheres to the required function signatures and produces a submission file when a test set is present.", "The LightGBM LGBMRegressor.fit call uses the argument early_stopping_rounds, which is not supported in the installed LightGBM version, causing a TypeError. Remove early_stopping_rounds (or switch to lgb.train with callbacks) and optionally adjust n_estimators. After fixing, the script should train and evaluate without errors.", "The script fails because LGBMRegressor.fit is called with an unsupported 'verbose' argument, causing a TypeError. Remove the 'verbose' parameter (or replace with a supported alternative) to allow the model to train. After fixing, the code will perform 5\u2011fold CV and report an OOF RMSE, then train on the full data and generate test predictions.", "The code runs successfully, training a LightGBM regressor with log-transformed target and achieving an out-of-fold RMSE of approximately 7.04 in 5-fold cross-validation. All required functions are correctly implemented and no bugs were detected.", "The implementation deviates from the required function signatures: `train` is defined with four parameters (including optional X_val and y_val) and expects a Series for y_train, whereas the specification requires exactly two arguments (X_train and y_train as DataFrames). This mismatch will cause import errors in the evaluation environment. To fix, redefine `train` to accept only `X_train: pd.DataFrame` and `y_train: pd.DataFrame`, handle the target conversion internally, and remove the extra parameters.", "The script runs successfully, training a LightGBM model with log-transformed target and achieving an out-of-fold RMSE of 6.42191 on 5-fold cross-validation. The required functions are correctly implemented and predictions are saved for the test set.", "The script successfully trains a LightGBM model with log-transformed target, performs 5-fold cross-validation, and achieves an out-of-fold RMSE of approximately 6.42 on the original scale. No bugs were detected, and the implementation conforms to the required function signatures.", "The solution implements the required train, prepare_val, predict, and run functions using LightGBM with a log-transformed target. A 5-fold cross-validation yields an out-of-fold RMSE of approximately 6.42, indicating reasonable predictive performance.", "The script runs without errors, performing a 5\u2011fold cross\u2011validation using LightGBM with a log1p target transformation. It achieves an out\u2011of\u2011fold RMSE of approximately 6.42, indicating reasonable predictive performance. The model is then retrained on the full data and generates predictions for the test set, saving them to working/submission.csv.", "The script trains a LightGBM regressor with log-transformed targets and evaluates via 5-fold cross-validation, achieving an OOF RMSE of 6.42191. All required functions are correctly implemented and no execution errors were observed.", "The script crashes because LGBMRegressor.fit is called with an unsupported argument 'early_stopping_rounds', leading to a TypeError. To fix this, remove the early_stopping_rounds parameter and optionally use LightGBM's callback API for early stopping, or simply train without early stopping. After fixing, the code should run and produce the reported 5\u2011Fold CV OOF RMSE.", "The script runs successfully, training a LightGBM model with log-transformed target and evaluating via 5-fold cross-validation. The out-of-fold RMSE is 6.42191, indicating reasonable predictive performance. No bugs were detected in the implementation.", "The script runs successfully, performing 5\u2011fold cross\u2011validation with LightGBM and achieving an OOF RMSE of approximately 6.42. The implementation adheres to the required function signatures and correctly handles log\u2011transformation of the target and categorical features.", "The script runs without errors and achieves an out-of-fold RMSE of approximately 6.42 on a 5\u2011fold cross\u2011validation. The required functions are correctly implemented with proper signatures, and predictions are generated and saved for the test set.", "The implementation correctly trains a LightGBM model with log-transformed target, handles categorical features, and produces predictions. The 5\u2011fold cross-validation yields an OOF RMSE of approximately 6.42, indicating reasonable performance on the dataset.", "The script successfully trains a LightGBM model with log-transformed target and evaluates using 5-fold cross-validation, achieving an OOF RMSE of approximately 6.42. It then retrains on the full data and generates predictions for the test set, saving them to a submission file.", "The script successfully trains a LightGBM regressor with log-transformed target, performs 5-fold cross-validation, and achieves an out-of-fold RMSE of approximately 6.42. No functional bugs were detected in the required API functions.", "The script successfully trains a LightGBM model with log-transformed target, performs 5\u2011fold cross\u2011validation, and achieves an OOF RMSE of approximately 6.42. The required functions (train, prepare_val, predict, run) are correctly implemented with proper signatures, and predictions for the test set are generated and saved without errors.", "The script runs without errors, trains a LightGBM model with log-transformed target, and achieves an out-of-fold RMSE of approximately 6.42 on 5-fold cross-validation.", "The train function signature includes extra optional parameters (X_val, y_val), which deviates from the required exact signature (train(X_train, y_train)). This will cause disqualification. Remove the extra arguments to match the specification. Empirically, the 5-fold CV RMSE achieved is 6.42191.", "The script successfully trains a LightGBM model with log-transformed target, performs 5-fold cross-validation, and achieves an out-of-fold RMSE of approximately 6.42. It then retrains on the full data and generates test predictions, saving them to working/submission.csv.", "The implementation adds extra optional parameters (X_val, y_val) to the required `train` function, violating the exact signature requirement and leading to disqualification. Remove these extra arguments to match the specified signature. The rest of the pipeline (categorical handling, log transformation, LightGBM model, and prediction logic) is sound and yields a reasonable CV RMSE (~6.42).", "The model trains and evaluates without errors, achieving an out-of-fold RMSE of approximately 13.91 on a 5\u2011fold cross\u2011validation. The pipeline correctly handles categorical encoding, log\u2011transforms the target, and produces predictions for the test set.", "The `train` function signature includes extra optional parameters (`X_val`, `y_val`) which deviates from the required exact signature of `train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any`. This will cause disqualification. Remove these extra arguments to match the specification.", "The model trained successfully using LightGBM with a log-transformed target. Five-fold cross-validation achieved an OOF RMSE of 6.42191, indicating reasonable predictive performance. The script also retrains on the full data and generates test predictions when a test set is present.", "The model trains successfully using LightGBM with a log-transformed target and achieves an out-of-fold RMSE of approximately 6.42 on 5-fold cross-validation. The implementation meets the required function signatures and produces predictions for the test set.", "The script correctly implements the required functions with proper signatures and runs a LightGBM model with log-transformed target. It performs 5-fold cross-validation achieving an OOF RMSE of ~6.42 and generates test predictions without errors.", "The implementation correctly trains a LightGBM model, performs 5\u2011fold CV, and outputs an OOF RMSE of 6.42191. However, the `train` function signature includes extra optional parameters (`X_val`, `y_val`) that deviate from the required exact signature of `train(X_train, y_train)`. This mismatch may cause disqualification. The fix is to remove the extra arguments and keep only the two required parameters.", "The script runs successfully, achieving an out-of-fold RMSE of 6.42191 on a 5\u2011fold cross\u2011validation. All required functions are correctly implemented with proper signatures, and no runtime errors were observed.", "The script successfully trains a LightGBM model with log\u2011transformed target, performs 5\u2011fold cross\u2011validation, and achieves an OOF RMSE of 6.42191. It also retrains on the full data and writes test predictions to a submission file when a test set is present.", "The model was trained using LightGBM with a log1p target transformation and categorical handling. 5\u2011fold cross\u2011validation yielded an OOF RMSE of approximately 6.42, indicating reasonable predictive performance. No execution errors or bugs were detected.", "The script successfully trains a LightGBM model with log-transformed target, performs 5-fold cross-validation, and achieves an out-of-fold RMSE of approximately 6.42. It also generates test predictions and saves a submission file when a test set is present.", "The script successfully trains a LightGBM model with log\u2011transformed target, performs 5\u2011fold cross\u2011validation and reports an OOF RMSE of 6.42191. All required functions are correctly implemented and no bugs were detected.", "The `train` function signature includes extra optional parameters (`X_val`, `y_val`) which deviates from the required exact signature `train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any`. This will cause disqualification. Remove these extra arguments to match the specification.", "The script successfully trains a LightGBM model with log-transformed target, performs 5-fold cross-validation, and reports an OOF RMSE of approximately 6.42. It then retrains on the full data and generates predictions for the test set, saving them to a submission file.", "The script runs without errors, performing 5\u2011fold cross\u2011validation and reporting an OOF RMSE of approximately 6.42. The model is then trained on the full data and predictions for the test set are saved to ./working/submission.csv.", "The script runs without errors, training a LightGBM model with log-transformed target and performing 5-fold cross-validation. The resulting out-of-fold RMSE is 6.70668, indicating reasonable predictive performance.", "The `train` function signature includes extra optional parameters (`X_val`, `y_val`) which deviates from the required exact signature `def train(X_train: pd.DataFrame, y_train: pd.DataFrame) -> Any`. This violates the competition rules and may cause disqualification. Remove these extra arguments to match the specification. The rest of the implementation (categorical handling, log\u2011transform, LightGBM model, and the other required functions) appears correct and achieves an OOF RMSE of ~6.42 on a 5\u2011fold split.", "The script correctly implements the required functions with proper signatures, trains a LightGBM model with log-transformed target, and achieves an out-of-fold RMSE of approximately 6.42. No evident bugs were found.", "The script runs without errors, training a LightGBM model with log\u2011transformed target and evaluating via 5\u2011fold CV. The resulting out\u2011of\u2011fold RMSE is approximately 6.42, indicating reasonable predictive performance.", "The implementation correctly defines the required functions with proper signatures, handles categorical features, applies log transformation to the target, and uses LightGBM for regression. The 5\u2011fold cross\u2011validation yields an OOF RMSE of approximately 6.71, indicating reasonable predictive performance.", "The implementation defines the required functions, but the `train` function signature includes extra optional parameters (`X_val`, `y_val`) which deviates from the exact required signature `train(X_train, y_train)`. This mismatch will cause a failure in the evaluation harness that expects the exact signature. Remove the extra parameters to match the specification. The empirical 5\u2011fold CV RMSE achieved is 6.42191.", "The script trains a LightGBM regressor with log\u2011transformed target, evaluates on a hold\u2011out split achieving RMSE ~5.95, and generates a submission file when test data is present. All required functions are correctly implemented and no execution errors were observed.", "The train function signature includes extra optional parameters (X_val, y_val) which deviates from the required exact signature (train(X_train, y_train)). This will cause disqualification. Fix by removing the optional arguments so the signature matches exactly. Apart from this, the model achieves an OOF RMSE of ~6.71 on 5\u2011fold CV.", "The script successfully trains a LightGBM regressor with log-transformed target, performs 5\u2011fold cross\u2011validation, and achieves an out\u2011of\u2011fold RMSE of 6.70668. It then retrains on the full data and generates predictions for the test set, saving them to a submission file.", "The script crashes because LGBMRegressor.fit() is called with the unsupported argument early_stopping_rounds, leading to a TypeError. Remove early_stopping_rounds (or replace with a callback) and optionally adjust validation handling. After fixing, the model will train and the CV RMSE can be reported.", "The code runs successfully, achieving a 5\u2011fold cross\u2011validated RMSE of approximately 6.42. The required functions are correctly implemented and a submission file is generated.", "The implementation deviates from the required function signatures: 'train' includes extra optional arguments and expects y_train as a Series, while the specification requires a DataFrame with exactly two parameters. Similarly, 'run' expects y_train as a Series instead of a DataFrame. Adjusting these signatures to match the specification (e.g., converting y_train DataFrame to a 1D array inside the function) will resolve the bug. Empirically, the 5\u2011fold cross\u2011validation achieved an OOF RMSE of ~6.71, indicating reasonable predictive performance.", "The script successfully trains a LightGBM regression model with log-target scaling, performs 5\u2011fold cross\u2011validation, and achieves an OOF RMSE of 6.70668. It then retrains on the full data and generates test predictions without errors.", "The script runs without errors, training a LightGBM regressor with log\u2011transformed target and evaluating via 5\u2011fold CV. The out\u2011of\u2011fold RMSE is 6.42, indicating reasonable predictive performance. The model is then retrained on the full data and test predictions are saved to ./working/submission.csv.", "The script achieved an out-of-fold RMSE of approximately 5.85, indicating reasonable predictive performance. However, the `train` function signature includes extra optional parameters (X_val, y_val) which deviate from the required exact signature `train(X_train, y_train)`, causing a bug.", "The script runs successfully, achieving an out-of-fold RMSE of approximately 5.85 on a 5\u2011fold cross\u2011validation. The required functions (train, prepare_val, predict, run) are correctly implemented and produce predictions for validation and test data.", "The script runs successfully, training a LightGBM model with 5\u2011fold cross\u2011validation and achieves an out\u2011of\u2011fold RMSE of approximately 5.85. Predictions for the test set are generated and saved without errors.", "The implementation violates the required function signature for `train`: it includes extra optional parameters `X_val` and `y_val`, which changes the expected signature. According to the competition rules, the `train` function must accept exactly two arguments (`X_train`, `y_train`). To fix, remove the extra parameters and handle any validation logic elsewhere (e.g., within `run` or a separate helper). The rest of the pipeline works and achieves an OOF RMSE of ~5.85.", "The script trains a LightGBM regressor, evaluates on a hold\u2011out split achieving an RMSE of ~5.66, and generates a submission file for the test set. All required functions are implemented with correct signatures and no runtime errors were observed.", "The model trained successfully using LightGBM with a log\u2011transformed target. On a 20% hold\u2011out split the RMSE was 5.94562, indicating reasonable predictive performance. The script also generated a submission file for the test set.", "The script successfully trains a LightGBM model with log\u2011transformed target, evaluates on a hold\u2011out split, and achieves an RMSE of approximately 6.23. No runtime errors or logical bugs were detected.", "The script runs without errors and achieves a hold\u2011out RMSE of ~6.53, but it saves the final predictions to a 'working/submission.csv' file. The competition requires the submission file to be placed in the 'submission/' directory. Update the output path to 'submission/submission.csv' (creating the directory if needed) to meet the submission specifications.", "The script runs successfully, performing 5\u2011fold cross\u2011validation with LightGBM and achieving a mean RMSE of approximately 6.99. It also trains on the full data and writes predictions to a submission file.", "The script runs successfully, training a LightGBM regressor and achieving a hold\u2011out RMSE of approximately 5.70. No errors or bugs were detected.", "The code runs successfully, training a LightGBM model with log-transformed target and generating predictions. On a hold-out validation split, it achieves an RMSE of approximately 6.53, and it also creates a submission file when test data is present.", "The script runs without errors, training a LightGBM regressor with log-transformed target and achieving a hold-out RMSE of 6.23414. Functions conform to required signatures and generate a submission file.", "The code fails because LGBMRegressor.fit() does not accept the 'early_stopping_rounds' argument in the installed LightGBM version, causing a TypeError. Remove this argument and optionally use callbacks for early stopping. After fixing, the script will perform 5\u2011fold CV and report the RMSE, then train on the full data and generate a submission file.", "The script runs successfully, training a LightGBM regressor on log-transformed target and achieving a hold-out RMSE of approximately 5.95. The required functions are correctly implemented and a submission file is generated when test data is present.", "The script successfully trains a LightGBM regressor with categorical handling and evaluates it using 5\u2011fold cross\u2011validation, achieving an average RMSE of approximately 5.85. No functional bugs were detected; the implementation meets the required interface and produces reasonable performance."], "exp_name": "0-en", "metrics": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
